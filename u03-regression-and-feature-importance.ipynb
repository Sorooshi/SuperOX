{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shap\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn import metrics\n",
    "from joblib import dump, load\n",
    "from Soroosh_utilities import *\n",
    "import tensorflow_probability as tfp\n",
    "from Soroosh_feature_importance import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "np.set_printoptions(suppress=True, precision=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfd = tfp.distributions\n",
    "tfpl = tfp.layers\n",
    "tfk = tf.keras\n",
    "tfkl = tf.keras.layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: GPU device not found.\n"
     ]
    }
   ],
   "source": [
    "if tf.test.gpu_device_name() != '/device:GPU:0':\n",
    "    print('WARNING: GPU device not found.')\n",
    "else:\n",
    "    print('SUCCESS: Found GPU: {}'.format(tf.test.gpu_device_name()))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The purposes of this set of studies:\n",
    "\n",
    "- We intent to predict the critical current, determine the important features, and visualize them.\n",
    "\n",
    "- Previously, it has been done for real-world data. In this set of studies, not only will we repeat those experiments, but also we use the synthetically generated data to increase our test samples. To this end, we pursue the following framework.\n",
    "\n",
    "    - 1) Train regressors on only real-world data;\n",
    "    - 2) Train regressors on only synthetic data;\n",
    "    - 3) Train regressors on combination of both (50%-50%).\n",
    "\n",
    "\n",
    "- To this end we use, three regression algorithms: a)RF, b) GBR-LS, c) DNNR.\n",
    "\n",
    "- Once the training is completed we will apply: 1) Gini-index, 2) Permutation, to determine the importance of features. Finally we use \"Partial dependence\" to visualize the impact of important features on the target values.\n",
    "\n",
    "\n",
    "- Note: For more see the notion page below\n",
    "\n",
    "https://www.notion.so/SuperOx-936b1b2ce7b14f20bd76578c82305e2b\n",
    "\n",
    "\n",
    "Note: we studied and tuned the parameters in the previous jupyter notebook. Thus we will use them as the default and given here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature importance:\n",
    "\n",
    "There exist several methods to determine the importance of features as follows.\n",
    "\n",
    "1. **The coefficients of linear regression:**\n",
    "\n",
    "    - Once the LR model is fitted, the weights can be used to interpret the features' importance.\n",
    "\n",
    "    - cons: over-simplistic.\n",
    "\n",
    "\n",
    "2. **Gini importance(or mean decrease in impurity) mechanism:** \n",
    "\n",
    "    - Gini importance is calculated as the decrease in node impurity weighted by the probability of reaching that node. The node probability can be calculated by the number of samples that reach the node, divided by the total number of samples. The higher the value, the more important the feature. \n",
    "    \n",
    "    In other words, the mean decrease in impurity importance of a feature is computed by measuring how effective the feature is at reducing uncertainty (classifiers) or variance (regressors) when creating decision trees within RFs.\n",
    "\n",
    "    - Cons: impurity-based feature importances can be misleading for high cardinality features. (It tends to inflate the importance of continuous or high-cardinality categorical variables). \n",
    "    \n",
    "\n",
    "3. **Permutation importance mechanism:** First, a baseline metric, defined by scoring, is evaluated on a (potentially different) dataset defined by the X. Next, a feature column from the validation set is permuted, and the metric is re-evaluated. The permutation importance is defined to be the difference between the baseline metric and metric from permutating the feature column.\n",
    "\n",
    "    - Cons: computationally, more expensive than Gini importance, but still more reliable. Moreover, highly correlated features could yield biased estimations or overestimations of the importance of features (Strobl et al. (2007), Nicodemus et al. (2010)).\n",
    "\n",
    "\n",
    "### Additive Feature Attribution Methods: (SHAP lib)\n",
    "\n",
    "The best explanation of a simple model is the model itself; it perfectly represents itself and is easy to understand. For complex models, such as ensemble methods or deep networks, we cannot use the original model as its own best explanation because it is not easy to understand. Instead, we must use a simpler explanation model, which we define as any interpretable approximation of the original model. We show below that six current explanation methods from the literature all use the same explanation model. This previously unappreciated unity has interesting implications, which we describe in later sections.\n",
    "\n",
    "Let $f$ be the original prediction model to be explained and $g$ the explanation model. \n",
    "Here, we focus on local methods designed to explain a prediction $f(x)$ based on a single input $x$. Explanation models often use simplified inputs $x′$ that map to the original inputs through a mapping function $x = h_x(x′)$. Local methods try to ensure $g(z′) ≈ f(h_x(z′))$ whenever $z′ ≈ x′$. (Note that $h_x(x′) = x$ even though $x′$ may contain less information than $x$ because $h_x$ is specific to the current input $x$.)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Definition.**\n",
    "\n",
    "Additive feature attribution methods have an explanation model that is a linear function of binary variables:\n",
    "\n",
    "$g(z^′)=φ_0 + \\sum_{i=1}^{M}φ_iz_i^{′} \\ \\ \\ Eqn.(1) $ \n",
    "\n",
    "\n",
    "where $z′ \\in \\{0, 1\\}^M$ , $M$ is the number of simplified input features, and $φ_i \\mathbb{R}$.\n",
    "\n",
    "\n",
    "\n",
    "4. **LIME:**\n",
    "\n",
    "The LIME method interprets individual model predictions based on locally approximating the model around a given prediction. \n",
    "The local linear explanation model that LIME uses adheres to Equation 1 exactly and is thus an additive feature attribution method.\n",
    "LIME refers to simplified inputs $x^′$ as “interpretable inputs,” and the mapping $x = h_x(x^′)$ converts a binary vector of interpretable inputs into the original input space. Different types of $h_x$ mappings are used for different input spaces. For bag of words text features, $h_x$ converts a vector of 1’s or 0’s (present or not) into the original word count if the simplified input is one, or zero if the simplified input is zero. For images, $h_x$ treats the image as a set of super pixels; it then maps 1 to leaving the super pixel as its original value and 0 to replacing the super pixel with an average of neighboring pixels (this is meant to represent being missing).\n",
    "\n",
    "\n",
    "\n",
    "5. **DeepLIFT:**\n",
    "\n",
    "DeepLIFT attributes to each input $x_i$ a value $C_{∆xi ∆y}$ that represents the effect of that input being set to a reference value as opposed to its original value. This means that for DeepLIFT, the mapping $x = h_x(x′)$ converts binary values into the original inputs, where 1 indicates that an input takes its original value, and 0 indicates that it takes the reference value. The reference value, though chosen by the user, represents a typical uninformative background value for the feature.\n",
    "DeepLIFT uses a \"summation-to-delta\" property that states:  $ \\sum_{i=1}^{n} C_{∆xi∆o} =∆o,$ \n",
    "\n",
    "\n",
    "where $o = f(x)$ is the model output, $∆_o = f(x) − f(r)$, $∆x_i = x_i − r_i$, and r is the reference input. If we let $φ_i = C_{∆xi∆o}$ and $φ_0 = f(r)$, then DeepLIFT’s explanation model matches Equation 1 and is thus another additive feature attribution method.\n",
    "\n",
    "\n",
    "6. **Layer-Wise Relevance Propagation:**\n",
    "\n",
    "The layer-wise relevance propagation method interprets the predictions of deep networks [1]. As noted by Shrikumar et al., this menthod is equivalent to DeepLIFT with the reference activations of all neurons fixed to zero. Thus, x = hx(x′) converts binary values into the original input space, where 1 means that an input takes its original value, and 0 means an input takes the 0 value. Layer-wise relevance propagation’s explanation model, like DeepLIFT’s, matches Equation 1.\n",
    "\n",
    "\n",
    "7. **Shapley regression values** \n",
    "are feature importances for linear models in the presence of multicollinearity. This method requires retraining the model on all feature subsets $S ⊆ F$ , where $F$ is the set of all features. It assigns an importance value to each feature that represents the effect on the model prediction of including that feature. To compute this effect, a model $f_{S∪\\{i\\}}$ is trained with that feature present, and another model $fS$ is trained with the feature withheld. Then, predictions from the two models are compared on the current input $f_{S∪\\{i\\}}(x_{S∪\\{i\\}}) − f_S(x_S)$, where $x_S$ represents the values of the input features in the set $S$. Since the effect of withholding a feature depends on other features in the model, the preceding differences are computed for all possible subsets $S ⊆ F \\ \\{i\\}$. The Shapley values are then computed and used as feature attributions. They are a weighted average of all possible differences (Eqn.(4) from [1]).\n",
    "\n",
    "\n",
    "8. **Shapley sampling values**\n",
    "are meant to explain any model by: (1) applying sampling approximations to Equation 4, and (2) approximating the effect of removing a variable from the model by integrating over samples from the training dataset. This eliminates the need to retrain the model and allows fewer than $2^{|F |}$ differences to be computed. Since the explanation model form of Shapley sampling values is the same as that for Shapley regression values, it is also an additive feature attribution method.\n",
    "\n",
    "9. **Quantitative input influence**\n",
    "is a broader framework that addresses more than feature attributions. However, as part of its method it independently proposes a sampling approximation to Shapley values that is nearly identical to Shapley sampling values. It is thus another additive feature attribution method.\n",
    "\n",
    "\n",
    "**SHAP (SHapley Additive exPlanation) Values**: in [1] it was shown that indeed, 4,5, 6 corresponds to 7,8, 9 in respect. Thus the authors combined them together and proposed a library called \"shap\" to this end, with set of handy tools which we will use in this study.\n",
    "\n",
    "10. **SHAP Feature Importance:** \n",
    "\n",
    "The idea behind SHAP feature importance is simple: Features with large absolute Shapley values are important. Since we want the global importance, we average the absolute Shapley values per feature across the data:\n",
    "\n",
    "$$\n",
    "I_j = \\sum_{j=1}^{n} |\\phi_j^{i}|\n",
    "$$\n",
    "\n",
    "\n",
    "Next, we sort the features by decreasing importance and plot them.\n",
    "\n",
    "### Conclusion over feature importance methods:\n",
    "\n",
    "- Since\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "[1]: Lundberg, S. and Lee, S.I., 2017. A unified approach to interpreting model predictions. arXiv preprint arXiv:1705.07874.\n",
    "\n",
    "for high-level description, see https://towardsdatascience.com/one-feature-attribution-method-to-supposedly-rule-them-all-shapley-values-f3e04534983d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializatin for comparison of methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MAE</th>\n",
       "      <th>MRAE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>R^2-Score</th>\n",
       "      <th>Value, Name-1</th>\n",
       "      <th>Value, Name-2</th>\n",
       "      <th>Value, Name-3</th>\n",
       "      <th>Value, Name-4</th>\n",
       "      <th>Value, Name-5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>RF-Gini</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RF-Perm</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RF-Shap</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GBR-Gini</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GBR-Perm</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GBR-Shap</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DNNR-Shap</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           MAE MRAE RMSE R^2-Score Value, Name-1 Value, Name-2 Value, Name-3  \\\n",
       "RF-Gini    NaN  NaN  NaN       NaN           NaN           NaN           NaN   \n",
       "RF-Perm    NaN  NaN  NaN       NaN           NaN           NaN           NaN   \n",
       "RF-Shap    NaN  NaN  NaN       NaN           NaN           NaN           NaN   \n",
       "GBR-Gini   NaN  NaN  NaN       NaN           NaN           NaN           NaN   \n",
       "GBR-Perm   NaN  NaN  NaN       NaN           NaN           NaN           NaN   \n",
       "GBR-Shap   NaN  NaN  NaN       NaN           NaN           NaN           NaN   \n",
       "DNNR-Shap  NaN  NaN  NaN       NaN           NaN           NaN           NaN   \n",
       "\n",
       "          Value, Name-4 Value, Name-5  \n",
       "RF-Gini             NaN           NaN  \n",
       "RF-Perm             NaN           NaN  \n",
       "RF-Shap             NaN           NaN  \n",
       "GBR-Gini            NaN           NaN  \n",
       "GBR-Perm            NaN           NaN  \n",
       "GBR-Shap            NaN           NaN  \n",
       "DNNR-Shap           NaN           NaN  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_algs = ['RF-Gini', 'RF-Perm', 'RF-Shap', 'GBR-Gini', 'GBR-Perm', 'GBR-Shap',\n",
    "            'DNNR-Shap']  # Regression algorithms ('Bayes_Reg')\n",
    "results = ['MAE', 'MRAE', 'RMSE', 'R^2-Score', \n",
    "           'Value, Name-1', 'Value, Name-2', 'Value, Name-3', \n",
    "           'Value, Name-4', 'Value, Name-5']  \n",
    "df_cmp_real = pd.DataFrame(index=reg_algs, columns=results) \n",
    "df_cmp_real\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MAE</th>\n",
       "      <th>MRAE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>R^2-Score</th>\n",
       "      <th>Value, Name-1</th>\n",
       "      <th>Value, Name-2</th>\n",
       "      <th>Value, Name-3</th>\n",
       "      <th>Value, Name-4</th>\n",
       "      <th>Value, Name-5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Baseline-Shap</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SDNNR-Shap</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SCNNR-Shap</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SRNNR-Shap</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               MAE MRAE RMSE R^2-Score Value, Name-1 Value, Name-2  \\\n",
       "Baseline-Shap  NaN  NaN  NaN       NaN           NaN           NaN   \n",
       "SDNNR-Shap     NaN  NaN  NaN       NaN           NaN           NaN   \n",
       "SCNNR-Shap     NaN  NaN  NaN       NaN           NaN           NaN   \n",
       "SRNNR-Shap     NaN  NaN  NaN       NaN           NaN           NaN   \n",
       "\n",
       "              Value, Name-3 Value, Name-4 Value, Name-5  \n",
       "Baseline-Shap           NaN           NaN           NaN  \n",
       "SDNNR-Shap              NaN           NaN           NaN  \n",
       "SCNNR-Shap              NaN           NaN           NaN  \n",
       "SRNNR-Shap              NaN           NaN           NaN  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_algs = [\"Baseline-Shap\", \"SDNNR-Shap\", \n",
    "            \"SCNNR-Shap\", \"SRNNR-Shap\", ]  # Regression algorithms ('Bayes_Reg')\n",
    "results = ['MAE', 'MRAE', 'RMSE', 'R^2-Score', \n",
    "           'Value, Name-1', 'Value, Name-2', 'Value, Name-3', \n",
    "           'Value, Name-4', 'Value, Name-5']  \n",
    "df_cmp_real_seq = pd.DataFrame(index=reg_algs, columns=results) \n",
    "df_cmp_real_seq\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MAE</th>\n",
       "      <th>MRAE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>R^2-Score</th>\n",
       "      <th>Value, Name-1</th>\n",
       "      <th>Value, Name-2</th>\n",
       "      <th>Value, Name-3</th>\n",
       "      <th>Value, Name-4</th>\n",
       "      <th>Value, Name-5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>RF-Gini</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RF-Perm</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GBR-Gini</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GBR-Perm</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DNNR-Gini</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DNNR-Perm</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           MAE MRAE RMSE R^2-Score Value, Name-1 Value, Name-2 Value, Name-3  \\\n",
       "RF-Gini    NaN  NaN  NaN       NaN           NaN           NaN           NaN   \n",
       "RF-Perm    NaN  NaN  NaN       NaN           NaN           NaN           NaN   \n",
       "GBR-Gini   NaN  NaN  NaN       NaN           NaN           NaN           NaN   \n",
       "GBR-Perm   NaN  NaN  NaN       NaN           NaN           NaN           NaN   \n",
       "DNNR-Gini  NaN  NaN  NaN       NaN           NaN           NaN           NaN   \n",
       "DNNR-Perm  NaN  NaN  NaN       NaN           NaN           NaN           NaN   \n",
       "\n",
       "          Value, Name-4 Value, Name-5  \n",
       "RF-Gini             NaN           NaN  \n",
       "RF-Perm             NaN           NaN  \n",
       "GBR-Gini            NaN           NaN  \n",
       "GBR-Perm            NaN           NaN  \n",
       "DNNR-Gini           NaN           NaN  \n",
       "DNNR-Perm           NaN           NaN  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_algs = ['RF-Gini', 'RF-Perm', 'GBR-Gini', 'GBR-Perm',\n",
    "            'DNNR-Gini', 'DNNR-Perm']  # Regression algorithms ('Bayes_Reg')\n",
    "\n",
    "results = ['MAE', 'MRAE', 'RMSE', 'R^2-Score', \n",
    "           'Value, Name-1', 'Value, Name-2', 'Value, Name-3', \n",
    "           'Value, Name-4', 'Value, Name-5']  \n",
    "\n",
    "df_cmp_synthetic = pd.DataFrame(index=reg_algs, columns=results) \n",
    "df_cmp_synthetic\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MAE</th>\n",
       "      <th>MRAE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>R^2-Score</th>\n",
       "      <th>Value, Name-1</th>\n",
       "      <th>Value, Name-2</th>\n",
       "      <th>Value, Name-3</th>\n",
       "      <th>Value, Name-4</th>\n",
       "      <th>Value, Name-5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>RF-Gini</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RF-Perm</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GBR-Gini</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GBR-Perm</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DNNR-Gini</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DNNR-Perm</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           MAE MRAE RMSE R^2-Score Value, Name-1 Value, Name-2 Value, Name-3  \\\n",
       "RF-Gini    NaN  NaN  NaN       NaN           NaN           NaN           NaN   \n",
       "RF-Perm    NaN  NaN  NaN       NaN           NaN           NaN           NaN   \n",
       "GBR-Gini   NaN  NaN  NaN       NaN           NaN           NaN           NaN   \n",
       "GBR-Perm   NaN  NaN  NaN       NaN           NaN           NaN           NaN   \n",
       "DNNR-Gini  NaN  NaN  NaN       NaN           NaN           NaN           NaN   \n",
       "DNNR-Perm  NaN  NaN  NaN       NaN           NaN           NaN           NaN   \n",
       "\n",
       "          Value, Name-4 Value, Name-5  \n",
       "RF-Gini             NaN           NaN  \n",
       "RF-Perm             NaN           NaN  \n",
       "GBR-Gini            NaN           NaN  \n",
       "GBR-Perm            NaN           NaN  \n",
       "DNNR-Gini           NaN           NaN  \n",
       "DNNR-Perm           NaN           NaN  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_algs = ['RF-Gini', 'RF-Perm', 'GBR-Gini', 'GBR-Perm',\n",
    "            'DNNR-Gini', 'DNNR-Perm']  # Regression algorithms ('Bayes_Reg')\n",
    "results = ['MAE', 'MRAE', 'RMSE', 'R^2-Score', \n",
    "           'Value, Name-1', 'Value, Name-2', 'Value, Name-3', \n",
    "           'Value, Name-4', 'Value, Name-5']\n",
    "\n",
    "df_cmp_combined = pd.DataFrame(index=reg_algs, columns=results) \n",
    "df_cmp_combined\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pld_complete = catalog.load('pld_complete').dropna().sort_values('pos', ascending=True)\n",
    "# pld_complete_zscore = pd.read_csv(\"/home/soroosh/SearchOX/data/pld_complete_zscore.csv\", index_col=False)\n",
    "\n",
    "\n",
    "pld_complete_range = pd.read_csv(\"/home/soroosh/Desktop/SearchOX/data/pld_complete_range.csv\",\n",
    "                                 index_col=False)\n",
    "\n",
    "pld_complete_range_synthetic = np.loadtxt(\"/home/soroosh/Desktop/SearchOX/data/x_r_synthetic.npy\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>median_Voltage_HSR_V_1025</th>\n",
       "      <th>median_Voltage_HSR_V_1027</th>\n",
       "      <th>median_Voltage_HSR_V_1030</th>\n",
       "      <th>median_Voltage_HSL_V_1025</th>\n",
       "      <th>median_Voltage_HSL_V_1027</th>\n",
       "      <th>median_Voltage_HSL_V_1030</th>\n",
       "      <th>median_Voltage_HF_V_1025</th>\n",
       "      <th>median_Voltage_HF_V_1027</th>\n",
       "      <th>median_Voltage_HF_V_1030</th>\n",
       "      <th>median_Voltage_HC_V_1025</th>\n",
       "      <th>...</th>\n",
       "      <th>std_Sigma_1030</th>\n",
       "      <th>pos</th>\n",
       "      <th>Speed</th>\n",
       "      <th>X FWHM</th>\n",
       "      <th>Y FWHM</th>\n",
       "      <th>R FWHM</th>\n",
       "      <th>Coolness</th>\n",
       "      <th>Coolness_neg</th>\n",
       "      <th>Ic</th>\n",
       "      <th>Ic_norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.129479</td>\n",
       "      <td>0.177414</td>\n",
       "      <td>0.005142</td>\n",
       "      <td>0.69562</td>\n",
       "      <td>-0.256257</td>\n",
       "      <td>-0.015406</td>\n",
       "      <td>-0.009215</td>\n",
       "      <td>-0.072807</td>\n",
       "      <td>-0.021102</td>\n",
       "      <td>0.057123</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.390667</td>\n",
       "      <td>-0.501695</td>\n",
       "      <td>0.009841</td>\n",
       "      <td>0.039589</td>\n",
       "      <td>0.067470</td>\n",
       "      <td>0.015697</td>\n",
       "      <td>0.053006</td>\n",
       "      <td>-0.086369</td>\n",
       "      <td>496.2</td>\n",
       "      <td>1.767913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.110059</td>\n",
       "      <td>0.340245</td>\n",
       "      <td>0.005142</td>\n",
       "      <td>0.69562</td>\n",
       "      <td>-0.321465</td>\n",
       "      <td>-0.015406</td>\n",
       "      <td>-0.009215</td>\n",
       "      <td>-0.173901</td>\n",
       "      <td>-0.021102</td>\n",
       "      <td>0.050178</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.390667</td>\n",
       "      <td>-0.501652</td>\n",
       "      <td>-0.005266</td>\n",
       "      <td>0.039589</td>\n",
       "      <td>0.067470</td>\n",
       "      <td>0.015697</td>\n",
       "      <td>0.053006</td>\n",
       "      <td>-0.086369</td>\n",
       "      <td>494.7</td>\n",
       "      <td>1.762568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.119769</td>\n",
       "      <td>0.340245</td>\n",
       "      <td>0.005142</td>\n",
       "      <td>0.69562</td>\n",
       "      <td>-0.321465</td>\n",
       "      <td>-0.015406</td>\n",
       "      <td>-0.007363</td>\n",
       "      <td>-0.173901</td>\n",
       "      <td>-0.021102</td>\n",
       "      <td>0.057123</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.390667</td>\n",
       "      <td>-0.501646</td>\n",
       "      <td>-0.007197</td>\n",
       "      <td>0.039589</td>\n",
       "      <td>0.067470</td>\n",
       "      <td>0.015697</td>\n",
       "      <td>0.053006</td>\n",
       "      <td>-0.086369</td>\n",
       "      <td>494.2</td>\n",
       "      <td>1.760787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.110059</td>\n",
       "      <td>0.348139</td>\n",
       "      <td>0.005142</td>\n",
       "      <td>0.69562</td>\n",
       "      <td>-0.322354</td>\n",
       "      <td>-0.015406</td>\n",
       "      <td>-0.009215</td>\n",
       "      <td>-0.218270</td>\n",
       "      <td>-0.021102</td>\n",
       "      <td>0.057123</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.390667</td>\n",
       "      <td>-0.501635</td>\n",
       "      <td>-0.010946</td>\n",
       "      <td>0.039589</td>\n",
       "      <td>0.067470</td>\n",
       "      <td>0.015697</td>\n",
       "      <td>0.053006</td>\n",
       "      <td>-0.086369</td>\n",
       "      <td>495.5</td>\n",
       "      <td>1.765419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.110059</td>\n",
       "      <td>0.264593</td>\n",
       "      <td>0.005142</td>\n",
       "      <td>0.69562</td>\n",
       "      <td>-0.305820</td>\n",
       "      <td>-0.015406</td>\n",
       "      <td>-0.009215</td>\n",
       "      <td>-0.129533</td>\n",
       "      <td>-0.021102</td>\n",
       "      <td>0.057123</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.390667</td>\n",
       "      <td>-0.501619</td>\n",
       "      <td>-0.017147</td>\n",
       "      <td>0.042682</td>\n",
       "      <td>0.069442</td>\n",
       "      <td>0.017956</td>\n",
       "      <td>0.051965</td>\n",
       "      <td>-0.084836</td>\n",
       "      <td>497.9</td>\n",
       "      <td>1.773168</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 379 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   median_Voltage_HSR_V_1025  median_Voltage_HSR_V_1027  \\\n",
       "0                   0.129479                   0.177414   \n",
       "1                   0.110059                   0.340245   \n",
       "2                   0.119769                   0.340245   \n",
       "3                   0.110059                   0.348139   \n",
       "4                   0.110059                   0.264593   \n",
       "\n",
       "   median_Voltage_HSR_V_1030  median_Voltage_HSL_V_1025  \\\n",
       "0                   0.005142                    0.69562   \n",
       "1                   0.005142                    0.69562   \n",
       "2                   0.005142                    0.69562   \n",
       "3                   0.005142                    0.69562   \n",
       "4                   0.005142                    0.69562   \n",
       "\n",
       "   median_Voltage_HSL_V_1027  median_Voltage_HSL_V_1030  \\\n",
       "0                  -0.256257                  -0.015406   \n",
       "1                  -0.321465                  -0.015406   \n",
       "2                  -0.321465                  -0.015406   \n",
       "3                  -0.322354                  -0.015406   \n",
       "4                  -0.305820                  -0.015406   \n",
       "\n",
       "   median_Voltage_HF_V_1025  median_Voltage_HF_V_1027  \\\n",
       "0                 -0.009215                 -0.072807   \n",
       "1                 -0.009215                 -0.173901   \n",
       "2                 -0.007363                 -0.173901   \n",
       "3                 -0.009215                 -0.218270   \n",
       "4                 -0.009215                 -0.129533   \n",
       "\n",
       "   median_Voltage_HF_V_1030  median_Voltage_HC_V_1025  ...  std_Sigma_1030  \\\n",
       "0                 -0.021102                  0.057123  ...       -0.390667   \n",
       "1                 -0.021102                  0.050178  ...       -0.390667   \n",
       "2                 -0.021102                  0.057123  ...       -0.390667   \n",
       "3                 -0.021102                  0.057123  ...       -0.390667   \n",
       "4                 -0.021102                  0.057123  ...       -0.390667   \n",
       "\n",
       "        pos     Speed    X FWHM    Y FWHM    R FWHM  Coolness  Coolness_neg  \\\n",
       "0 -0.501695  0.009841  0.039589  0.067470  0.015697  0.053006     -0.086369   \n",
       "1 -0.501652 -0.005266  0.039589  0.067470  0.015697  0.053006     -0.086369   \n",
       "2 -0.501646 -0.007197  0.039589  0.067470  0.015697  0.053006     -0.086369   \n",
       "3 -0.501635 -0.010946  0.039589  0.067470  0.015697  0.053006     -0.086369   \n",
       "4 -0.501619 -0.017147  0.042682  0.069442  0.017956  0.051965     -0.084836   \n",
       "\n",
       "      Ic   Ic_norm  \n",
       "0  496.2  1.767913  \n",
       "1  494.7  1.762568  \n",
       "2  494.2  1.760787  \n",
       "3  495.5  1.765419  \n",
       "4  497.9  1.773168  \n",
       "\n",
       "[5 rows x 379 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pld_complete_range.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## splitting the sequential data\n",
    "\n",
    "### Real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((18561, 376), (18561,), (18561,), (18561,))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_r_real = pld_complete_range.loc[:,\n",
    "                                  ~pld_complete_range.columns.isin(['Ic', 'Ic_norm', 'pos'])]  # .to_numpy()\n",
    "\n",
    "y_ic_real = pld_complete_range['Ic']  # .to_numpy()\n",
    "y_ic_norm_real = pld_complete_range['Ic_norm']  # .to_numpy()\n",
    "pos_real = pld_complete_range['pos']  # .to_numpy()\n",
    "\n",
    "x_r_real.shape, y_ic_real.shape, y_ic_norm_real.shape, pos_real.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(370, 377, 378)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_idx = pld_complete_range.columns.get_loc(\"pos\")\n",
    "ic_idx = pld_complete_range.columns.get_loc(\"Ic\")\n",
    "ic_norm_idx = pld_complete_range.columns.get_loc(\"Ic_norm\")\n",
    "pos_idx, ic_idx, ic_norm_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 1000  # previously, it was 500\n",
    "learning_rate = 1e-5  # [1e-2, 1e-3, 1e-5] \n",
    "batch_size = 64  # [32, 64, 256]\n",
    "n_units = 128\n",
    "\n",
    "input_shape = (x_r_real.shape[1])  \n",
    "n_units = 128\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save synthetic data as df and then modify the two (four) cell below "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_r_synthetic_1 = pld_complete_range_synthetic[:, : pos_idx]\n",
    "# x_r_synthetic_2 = pld_complete_range_synthetic[:, pos_idx+1:ic_idx]\n",
    "\n",
    "\n",
    "# x_r_synthetic = np.concatenate([x_r_synthetic_1, x_r_synthetic_2], axis=1)\n",
    "# x_r_synthetic_1.shape, x_r_synthetic_2.shape, x_r_synthetic.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_ic_synthetic = pld_complete_range_synthetic[:, ic_idx]\n",
    "# y_ic_norm_synthetic = pld_complete_range_synthetic[:, ic_norm_idx] \n",
    "# pos_synthetic = pld_complete_range_synthetic[:, pos_idx]\n",
    "\n",
    "# x_r_synthetic.shape, y_ic_synthetic.shape, y_ic_norm_synthetic.shape, pos_synthetic.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_r_combined = np.concatenate([x_r_real, x_r_synthetic], axis=0)\n",
    "# y_ic_combined = np.concatenate([y_ic_real, y_ic_synthetic], axis=0)\n",
    "# y_ic_norm_combined = np.concatenate([y_ic_norm_real, y_ic_norm_synthetic], axis=0)\n",
    "\n",
    "# x_r_combined.shape, y_ic_combined.shape, y_ic_norm_combined.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert not np.any(np.isnan(x_r_real))\n",
    "# assert not x_r_real.shape != x_r_synthetic.shape\n",
    "# assert not x_r_combined.shape[0] != int(2*x_r_synthetic.shape[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## splitting the non-sequential data\n",
    "\n",
    "### Real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((11136, 376), (3713, 376), (3712, 376), (11136,), (3713,), (3712,))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_r_train_real, x_r_test_real, \\\n",
    "y_train_real, y_test_real = train_test_split(x_r_real,\n",
    "                                             y_ic_norm_real,\n",
    "                                             test_size=0.40,\n",
    "                                             random_state=43,)\n",
    "\n",
    "x_r_val_real, x_r_test_real, \\\n",
    "y_val_real, y_test_real = train_test_split(x_r_test_real,\n",
    "                                           y_test_real,\n",
    "                                           test_size=0.5,\n",
    "                                           random_state=43,)\n",
    "\n",
    "\n",
    "x_r_train_real.shape, x_r_test_real.shape, \\\n",
    "x_r_val_real.shape, y_train_real.shape, \\\n",
    "y_test_real.shape, y_val_real.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((11136, 377), (3713, 376), (3712, 377), (11136,), (3713, 377), (3712,))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pld_complete_range_ = pld_complete_range.loc[:,\n",
    "                                             ~pld_complete_range.columns.isin(['Ic', 'pos'])]\n",
    "\n",
    "column_indices = {name: i for i, name in enumerate(pld_complete_range_.columns)}\n",
    "\n",
    "n = len(x_r_real)\n",
    "x_r_train_real_seq = pld_complete_range_[0:int(n*0.6)]\n",
    "y_train_real_seq = y_ic_norm_real[0:int(n*0.6)]\n",
    "\n",
    "x_r_val_real_seq = pld_complete_range_[int(n*0.6):int(n*0.8)]\n",
    "y_val_real_seq = y_ic_norm_real[int(n*0.6):int(n*0.8)]\n",
    "\n",
    "\n",
    "x_r_test_real_seq = x_r_real[int(n*0.8):]\n",
    "y_test_real_seq = pld_complete_range_[int(n*0.8):]\n",
    "\n",
    "\n",
    "\n",
    "x_r_train_real_seq.shape, x_r_test_real_seq.shape, \\\n",
    "x_r_val_real_seq.shape, y_train_real_seq.shape, \\\n",
    "y_test_real_seq.shape, y_val_real_seq.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synthetic data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_r_train_synthetic, x_r_test_synthetic,\\\n",
    "# y_train_synthetic, y_test_synthetic = train_test_split(x_r_synthetic,\n",
    "#                                                        y_ic_norm_synthetic,\n",
    "#                                                        test_size=0.40,\n",
    "#                                                        random_state=43,)\n",
    "\n",
    "# x_r_val_synthetic, x_r_test_synthetic,\\\n",
    "# y_val_synthetic, y_test_synthetic = train_test_split(x_r_test_synthetic,\n",
    "#                                                      y_test_synthetic,\n",
    "#                                                      test_size=0.5,\n",
    "#                                                      random_state=43,)\n",
    "\n",
    "\n",
    "# x_r_train_synthetic.shape, x_r_test_synthetic.shape, \\\n",
    "# x_r_val_synthetic.shape, y_train_synthetic.shape,  y_test_synthetic.shape, y_val_synthetic.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combined data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_r_train_combined, x_r_test_combined, \\\n",
    "# y_train_combined, y_test_combined = train_test_split(x_r_combined,\n",
    "#                                                      y_ic_norm_combined,\n",
    "#                                                      test_size=0.40,\n",
    "#                                                      random_state=43,)\n",
    "\n",
    "# x_r_val_combined, x_r_test_combined, \\\n",
    "# y_val_combined, y_test_combined = train_test_split(x_r_test_combined,\n",
    "#                                                    y_test_combined,\n",
    "#                                                    test_size=0.5,\n",
    "#                                                    random_state=43,)\n",
    "\n",
    "\n",
    "# x_r_train_combined.shape, x_r_test_combined.shape, \\\n",
    "# x_r_val_combined.shape, y_train_combined.shape, \\\n",
    "# y_test_combined.shape, y_val_combined.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determining the importance of features\n",
    "A) Permutation\n",
    "B) Gini-index\n",
    "C)Shap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training RF\n",
    "\n",
    "- For more about tuning the parameters see the u01-*.ipynb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_eval_rf(x_train, y_train, x_test, y_test, name):\n",
    "    \n",
    "    rf_reg = RandomForestRegressor(n_estimators=100, \n",
    "                                   n_jobs=-2, \n",
    "                                   criterion='mse', \n",
    "                                   min_samples_leaf=1,\n",
    "                                   verbose=1)\n",
    "    \n",
    "    rf_reg.fit(x_train, y_train)\n",
    "    \n",
    "    y_preds_rf = rf_reg.predict(x_test)\n",
    "    \n",
    "    filename = \"rf_reg-\" + name +  \"-.joblib\"\n",
    "    dump(rf_reg, \"saved_model/\"+ filename )\n",
    "    \n",
    "    features_importance_gini = rf_reg.feature_importances_\n",
    "    \n",
    "    features_importance_perm = permutation_importance(estimator=rf_reg,\n",
    "                                                      X=x_test, y=y_test,\n",
    "                                                      n_repeats=5,\n",
    "                                                      n_jobs=-2,\n",
    "                                                      random_state=0)\n",
    "    \n",
    "    explainer, features_importance_shap = apply_shap_summary_plot(model=rf_reg,\n",
    "                                                      x_train=x_train, y_train=y_train, \n",
    "                                                      x_test=x_test, y_test=y_test, \n",
    "                                                      n_clusters=10,\n",
    "                                                      model_name=\"RF\", data_type=name\n",
    "                                                     )\n",
    "    \n",
    "    \n",
    "    return y_preds_rf, features_importance_gini, features_importance_perm, features_importance_shap\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Gradient Boosting Regressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_eval_gbr(x_train, y_train, x_test, y_test, name):\n",
    "    \n",
    "    gbr_ls = GradientBoostingRegressor(loss='ls', verbose=1,)\n",
    "    gbr_ls.fit(x_train, y_train)\n",
    "    y_preds_gbr_ls = gbr_ls.predict(x_test)\n",
    "    \n",
    "    features_importance_gini = gbr_ls.feature_importances_\n",
    "    \n",
    "    features_importance_perm = permutation_importance(estimator=gbr_ls,\n",
    "                                                     X=x_test, y=y_test,\n",
    "                                                     n_repeats=5,\n",
    "                                                     n_jobs=-2,\n",
    "                                                     random_state=0)\n",
    "    \n",
    "    filename = \"gbr_ls-\" + name +  \"-.joblib\"\n",
    "    dump(gbr_ls, \"saved_model/\"+ filename )\n",
    "        \n",
    "    \n",
    "    explainer, features_importance_shap = apply_shap_summary_plot(model=gbr_ls,\n",
    "                                                      x_train=x_train, y_train=y_train, \n",
    "                                                      x_test=x_test, y_test=y_test, \n",
    "                                                      n_clusters=10,\n",
    "                                                      model_name=\"GBR\", data_type=name\n",
    "                                                     )\n",
    "    \n",
    "#     features_importance_shap = shap_importance(shap_values, x_test)\n",
    "\n",
    "    \n",
    "    return y_preds_gbr_ls, features_importance_gini, features_importance_perm, features_importance_shap\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DNN-Regressor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DnnReg(tfk.Model):\n",
    "    \n",
    "    def __init__(self, n_units, n_features, name='dnn_reg', **kwargs):\n",
    "        super(DnnReg, self).__init__(name=name, **kwargs)\n",
    "        self.n_units = n_units\n",
    "        self.n_features = n_features\n",
    "        \n",
    "        self.input_layer = tfkl.InputLayer(input_shape=self.n_features)\n",
    "        self.cast_layer = tfkl.Lambda(lambda x: tf.cast(x, tf.float32))\n",
    "        self.dense_1 = tfkl.Dense(units=int(.5*self.n_units), activation=tf.nn.leaky_relu)\n",
    "        self.dense_2 = tfkl.Dense(units=self.n_units, activation=tf.nn.leaky_relu)\n",
    "        self.dense_3 = tfkl.Dense(units=2*self.n_units, activation=tf.nn.leaky_relu,)\n",
    "        self.dense_4 = tfkl.Dense(units=4*self.n_units, activation=tf.nn.leaky_relu,)\n",
    "        self.regressor = tfkl.Dense(units=1)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = self.input_layer(inputs)\n",
    "        x = self.cast_layer(x)\n",
    "        x = self.dense_1(x)\n",
    "        x = self.dense_2(x)\n",
    "        x = self.dense_3(x)\n",
    "        x = self.dense_4(x)\n",
    "        regression = self.regressor(x)\n",
    "        return regression\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_eval_dnn_reg(dnn_reg, x_train, y_train, x_val, y_val, x_test, y_test, name):\n",
    "\n",
    "    callback = tfk.callbacks.EarlyStopping(monitor='loss', patience=5)  # for early-stop\n",
    "\n",
    "    dnn_reg.compile(optimizer=tf.optimizers.Adam(learning_rate=learning_rate),\n",
    "                    loss='mse',\n",
    "                   )\n",
    "\n",
    "    history_dnn_ref = dnn_reg.fit(x=x_train, y=y_train,\n",
    "                                  epochs=n_epochs, batch_size=batch_size,\n",
    "                                  validation_data=(x_val, y_val),\n",
    "                                  # callbacks=[callback],\n",
    "                                 )\n",
    "\n",
    "\n",
    "#     plot_loss(history=history_dnn_ref, name='DNN-Reg('+ \n",
    "#               str(batch_size)+ \", \" + str(learning_rate)+ ')')\n",
    "\n",
    "    # Saving the trained weights for future applications\n",
    "    # !mkdir -p saved_model\n",
    "    filename = \"dnn-reg-\" + name +  \"-.joblib\"\n",
    "    dnn_reg.save_weights('saved_model/' + filename + '.h5')\n",
    "\n",
    "    y_preds_dnn_reg = dnn_reg.predict(x_test)\n",
    "    \n",
    "    \n",
    "    explainer, features_importance_shap = apply_shap_summary_plot(model=dnn_reg,\n",
    "                                                     x_train=x_train, \n",
    "                                                     y_train=y_train, \n",
    "                                                     x_test=x_test, \n",
    "                                                     y_test=y_test, \n",
    "                                                     n_clusters=10,\n",
    "                                                     model_name=\"DNNR\", \n",
    "                                                     data_type=name\n",
    "                                                    )\n",
    "    \n",
    "#     features_importance_shap = shap_importance(shap_values[0], x_test)  # bcz it is a list of np.arr\n",
    "    \n",
    "    print(\"y_preds_dnn_reg:\", y_preds_dnn_reg.shape)\n",
    "    \n",
    "    return y_preds_dnn_reg, features_importance_shap\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real-only data \n",
    "\n",
    "## RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rnd_sample = np.random.randint(0, 10000, size=(1000))\n",
    "\n",
    "x_r_train_real = x_r_train_real.iloc[rnd_sample,:]\n",
    "y_train_real = y_train_real.iloc[rnd_sample]\n",
    "\n",
    "rnd_sample = np.random.randint(0, 3000, size=(500))\n",
    "x_r_val_real = x_r_val_real.iloc[rnd_sample, :]\n",
    "y_val_real = y_val_real.iloc[rnd_sample]\n",
    "\n",
    "rnd_sample = np.random.randint(0, 3000, size=(500))\n",
    "x_r_test_real = x_r_test_real.iloc[rnd_sample, ]\n",
    "y_test_real = y_test_real.iloc[rnd_sample]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 376) (500, 376) (500, 376) (1000,) (500,) (500,)\n"
     ]
    }
   ],
   "source": [
    "print(x_r_train_real.shape, x_r_val_real.shape, x_r_test_real.shape, \n",
    "      y_train_real.shape, y_val_real.shape, y_test_real.shape, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# rnd_sample = np.random.choice(train_df_real.index, 100)\n",
    "\n",
    "\n",
    "\n",
    "train_df = pld_complete_range_.iloc[:1000, :]\n",
    "val_df = pld_complete_range_.iloc[1000:1500, :]\n",
    "test_df = pld_complete_range_.iloc[1500:2000, :]\n",
    "\n",
    "\n",
    "# x_r_train_real_seq = x_r_train_real_seq.iloc[:100, 370:]\n",
    "# y_train_real_seq = y_train_real_seq.iloc[:100, ]\n",
    "# x_r_val_real_seq = x_r_val_real_seq.iloc[:100, 370:]\n",
    "# y_val_real_seq = y_val_real_seq.iloc[:100,]\n",
    "# x_r_test_real_seq = x_r_test_real_seq.iloc[:100, 370:]\n",
    "# y_test_real_seq = y_test_real_seq.iloc[:100, ]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1000, 377), (500, 377), (500, 377))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape, test_df.shape, val_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_eval_a_nonsequential_regressor(x_train, y_train, x_val, y_val, x_test, \n",
    "                                         y_test, df_cmp, algorithm, data_type):\n",
    "    \n",
    "    if algorithm.lower() == \"rf\":\n",
    "\n",
    "        y_preds, features_importance_gini, \\\n",
    "        features_importance_perm, \\\n",
    "        features_importance_shap = train_eval_rf(x_train=x_train,\n",
    "                                                 y_train=y_train, \n",
    "                                                 x_test=x_test,\n",
    "                                                 y_test=y_test,\n",
    "                                                 name=data_type\n",
    "                                                )\n",
    "        \n",
    "    elif algorithm.lower() == \"gbr\":\n",
    "        \n",
    "        y_preds, features_importance_gini, \\\n",
    "        features_importance_perm, \\\n",
    "        features_importance_shap = train_eval_gbr(x_train=x_train,\n",
    "                                                  y_train=y_train, \n",
    "                                                  x_test=x_test,\n",
    "                                                  y_test=y_test,\n",
    "                                                  name=data_type\n",
    "                                                 )\n",
    "        \n",
    "    elif algorithm.lower() == \"dnnr\":\n",
    "        \n",
    "        dnn_reg = DnnReg(n_units=n_units, n_features=[x_train.shape[1]])\n",
    "        n_features=[x_train.shape[1]]\n",
    "        print(\"n_features=[x_train.shape[1]]:\", n_features)\n",
    "        \n",
    "        y_preds, features_importance_shap = train_eval_dnn_reg(dnn_reg=dnn_reg, \n",
    "                                                               x_train=x_train, \n",
    "                                                               y_train=y_train, \n",
    "                                                               x_test=x_test,\n",
    "                                                               y_test=y_test,\n",
    "                                                               x_val=x_val,\n",
    "                                                               y_val=y_val,\n",
    "                                                               name=data_type\n",
    "                                                              )\n",
    "        \n",
    "        shap_indices = features_importance_shap[0]\n",
    "        shap_val = features_importance_shap[1]\n",
    "        shap_name = features_importance_shap[2]\n",
    "\n",
    "\n",
    "        df_cmp = final_comparison(df=df_cmp, \n",
    "                                   y_preds=y_preds,\n",
    "                                   y_trues=y_test, \n",
    "                                   name=algorithm+'-Shap',\n",
    "                                   data_name=data_type,\n",
    "                                   indices=shap_indices,\n",
    "                                   values_features_importance=shap_val,\n",
    "                                   name_important_features=shap_name\n",
    "                                  )\n",
    "        print(\" Shap is done!\")\n",
    "        \n",
    "\n",
    "    if algorithm.lower() == 'rf' or algorithm.lower() == 'gbr':\n",
    "        \n",
    "\n",
    "        gini_indices, gini_val, \\\n",
    "        gini_name = apply_gini(estimator_important_features=features_importance_gini,\n",
    "                               pld_complete=x_test, name=algorithm\n",
    "                              )\n",
    "\n",
    "\n",
    "        df_cmp = final_comparison(df=df_cmp, \n",
    "                                  y_preds=y_preds,\n",
    "                                  y_trues=y_test, \n",
    "                                  name=algorithm+'-Gini',\n",
    "                                  data_name=data_type,\n",
    "                                  indices=gini_indices,\n",
    "                                  values_features_importance=gini_val,\n",
    "                                  name_important_features=gini_name\n",
    "                                 )\n",
    "\n",
    "        print(\" Gini is done!\")\n",
    "\n",
    "        perm_indices, perm_val, \\\n",
    "        perm_name = apply_permutation(estimator_important_features=features_importance_perm,\n",
    "                                      pld_complete=x_test, name=algorithm)\n",
    "\n",
    "\n",
    "\n",
    "        df_cmp = final_comparison(df=df_cmp, \n",
    "                                  y_preds=y_preds,\n",
    "                                  y_trues=y_test, \n",
    "                                  name=algorithm+'-Perm',\n",
    "                                  data_name=data_type,\n",
    "                                  indices=perm_indices,\n",
    "                                  values_features_importance=perm_val,\n",
    "                                  name_important_features=perm_name\n",
    "                                 )\n",
    "        \n",
    "        print(\" Perm. is done!\")\n",
    "\n",
    "\n",
    "        \n",
    "        shap_indices = features_importance_shap[0]\n",
    "        shap_val = features_importance_shap[1]\n",
    "        shap_name = features_importance_shap[2]\n",
    "\n",
    "        df_cmp = final_comparison(df=df_cmp, \n",
    "                                   y_preds=y_preds,\n",
    "                                   y_trues=y_test, \n",
    "                                   name=algorithm+'-Shap',\n",
    "                                   data_name=data_type,\n",
    "                                   indices=shap_indices,\n",
    "                                   values_features_importance=shap_val,\n",
    "                                   name_important_features=shap_name\n",
    "                                  )\n",
    "        print(\" Shap is done!\")\n",
    "\n",
    "\n",
    "    return df_cmp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-2)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=-2)]: Done  20 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=-2)]: Done 100 out of 100 | elapsed:    1.5s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe3c17066f5d4fbc9823328e4e6b1cbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=15)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=15)]: Done 100 out of 100 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tmp = train_eval_a_nonsequential_regressor(x_train=x_r_train_real,\n",
    "                         y_train=y_train_real, \n",
    "                         x_val=x_r_val_real, \n",
    "                         y_val=y_val_real,\n",
    "                         x_test=x_r_test_real,\n",
    "                         y_test=y_test_real, \n",
    "                         df_cmp=df_cmp_real,\n",
    "                         algorithm=\"RF\", \n",
    "                         data_type=\"real-\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "tmp = train_eval_a_nonsequential_regressor(x_train=x_r_train_real,\n",
    "                                           y_train=y_train_real, \n",
    "                                           x_val=x_r_val_real,\n",
    "                                           y_val=y_val_real,\n",
    "                                           x_test=x_r_test_real, \n",
    "                                           y_test=y_test_real, \n",
    "                                           df_cmp=df_cmp_real,\n",
    "                                           algorithm=\"GBR\", \n",
    "                                           data_type=\"real-\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "tmp = train_eval_a_nonsequential_regressor(x_train=x_r_train_real, \n",
    "                                      y_train=y_train_real, \n",
    "                                      x_val=x_r_val_real,\n",
    "                                      y_val=y_val_real,\n",
    "                                      x_test=x_r_test_real,\n",
    "                                      y_test=y_test_real, \n",
    "                                      df_cmp=df_cmp_real,\n",
    "                                      algorithm=\"DNNR\", \n",
    "                                      data_type=\"real-\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cmp_real = pd.read_csv(\"final_comparison-real-.csv\")\n",
    "df_cmp_real"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequential Regressions\n",
    "\n",
    "We will make a set of predictions based on a window of consecutive samples from the data. More precisely, we will implement various models --including Sequential Dense NN Reg. (SDNNR), Sequential Convolutional NN Reg. (SCNNR), and Recurrent NN Reg. (SRNNR) models -- and predict based on a window of consecutive samples. So let us first implement this Window.\n",
    "\n",
    "\n",
    "### Data windowing\n",
    "\n",
    "\n",
    "The features of the input windows are:\n",
    "\n",
    "- The width (number of time steps) of the input and label windows\n",
    "- The time offset between them.\n",
    "- Which features are used as inputs, labels, or both.\n",
    "- Single-output and multi-output predictions.\n",
    "- Single-time-step and multi-time-step predictions.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WindowGenerator():\n",
    "    def __init__(self, input_width, label_width, shift,\n",
    "                 train_df, val_df, test_df,\n",
    "                 label_columns=None):\n",
    "        \n",
    "        # Store the raw data.\n",
    "        self.train_df = train_df\n",
    "        self.val_df = val_df\n",
    "        self.test_df = test_df\n",
    "\n",
    "        # Work out the label column indices.\n",
    "        self.label_columns = label_columns\n",
    "        \n",
    "        if label_columns is not None:\n",
    "            self.label_columns_indices = {name: i for i, name in\n",
    "                                          enumerate(label_columns)}\n",
    "        \n",
    "        self.column_indices = {name: i for i, name in\n",
    "                               enumerate(train_df.columns)}\n",
    "\n",
    "        # Work out the window parameters.\n",
    "        self.input_width = input_width\n",
    "        self.label_width = label_width\n",
    "        self.shift = shift\n",
    "\n",
    "        self.total_window_size = input_width + shift\n",
    "\n",
    "        self.input_slice = slice(0, input_width)\n",
    "        self.input_indices = np.arange(self.total_window_size)[self.input_slice]\n",
    "\n",
    "        self.label_start = self.total_window_size - self.label_width\n",
    "        self.labels_slice = slice(self.label_start, None)\n",
    "        self.label_indices = np.arange(self.total_window_size)[self.labels_slice]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '\\n'.join([\n",
    "            f'Total window size: {self.total_window_size}',\n",
    "            f'Input indices: {self.input_indices}',\n",
    "            f'Label indices: {self.label_indices}',\n",
    "            f'Label column name(s): {self.label_columns}'])\n",
    "    \n",
    "    def split_window(self, features):\n",
    "        inputs = features[:, self.input_slice, :]\n",
    "        labels = features[:, self.labels_slice, :]\n",
    "        if self.label_columns is not None:\n",
    "            labels = tf.stack(\n",
    "                [labels[:, :, self.column_indices[name]] for name in self.label_columns],\n",
    "                axis=-1)\n",
    "\n",
    "      # Slicing doesn't preserve static shape information, so set the shapes\n",
    "      # manually. This way the `tf.data.Datasets` are easier to inspect.\n",
    "        inputs.set_shape([None, self.input_width, None])\n",
    "        labels.set_shape([None, self.label_width, None])\n",
    "        \n",
    "        return inputs, labels\n",
    "    \n",
    "    \n",
    "    def make_dataset(self, data):\n",
    "        data = np.array(data, dtype=np.float32)\n",
    "        ds = tf.keras.preprocessing.timeseries_dataset_from_array(\n",
    "            data=data,\n",
    "            targets=None,\n",
    "            sequence_length=self.total_window_size,\n",
    "            sequence_stride=1,\n",
    "            shuffle=True,\n",
    "            batch_size=32,)\n",
    "        \n",
    "        ds = ds.map(self.split_window)\n",
    "        return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_windowed_tf_to_numpy(windowed_data_df):\n",
    "    \n",
    "    # Converting data TF to Numpy array for using in SHAP Lib. (Not very efficient way! :( )\n",
    "    # train data:\n",
    "    data_np_y = []\n",
    "    tmp_data = []\n",
    "    \n",
    "    for i, j in windowed_data_df:\n",
    "        data_np_y += j.numpy().flatten().tolist()\n",
    "        tmp_data.append(i.numpy())\n",
    "\n",
    "    bs, w, f = tmp_data[0].shape  # batch_size, window_size, n_features\n",
    "    n = sum([l.shape[0] for l in tmp_data])  # n_datapoints\n",
    "\n",
    "    data_np_x = np.empty(shape=(n, w, f))\n",
    "    interval = 0\n",
    "\n",
    "    for t in range(len(tmp_data)):\n",
    "        I, J, K = tmp_data[t].shape\n",
    "        for i in range(I):\n",
    "            for j in range(J):\n",
    "                for k in range(K):\n",
    "                    data_np_x[interval+i, j, k] = tmp_data[t][i, j, k]\n",
    "        interval += I\n",
    "        \n",
    "    return data_np_x, data_np_y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_and_fit(model, window):\n",
    "    \n",
    "    model.compile(loss=tf.losses.MeanSquaredError(),\n",
    "                optimizer=tf.optimizers.Adam(),\n",
    "                metrics=[tf.metrics.MeanAbsoluteError()])\n",
    "\n",
    "    history = model.fit(window.make_dataset(train_df), epochs=n_epochs,\n",
    "                      validation_data=window.make_dataset(val_df),\n",
    "                       )\n",
    "    \n",
    "    y_preds = model.predict(window.make_dataset(test_df))\n",
    "    y_preds = y_preds.flatten()\n",
    "    \n",
    "    x_train_np, y_train_np = convert_windowed_tf_to_numpy(window.make_dataset(train_df))\n",
    "    x_test_np, y_test_np = convert_windowed_tf_to_numpy(window.make_dataset(test_df))\n",
    "    \n",
    "    return history, y_preds, y_train_np, x_train_np, y_test_np, x_test_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_eval_a_sequential_regressor(train_df,\n",
    "                                      val_df, \n",
    "                                      test_df,\n",
    "                                      input_width, \n",
    "                                      label_width,\n",
    "                                      shift,\n",
    "                                      algorithm, \n",
    "                                      df_cmp, \n",
    "                                      data_type,\n",
    "                                      label_columns):\n",
    "    \n",
    "    \n",
    "\n",
    "    # Model instantiation\n",
    "    if algorithm == \"Baseline\":\n",
    "        print(\"Baseline is Running\")\n",
    "        \n",
    "        model = tfk.Sequential([\n",
    "            tfkl.Dense(units=1)\n",
    "        ])\n",
    "    \n",
    "    \n",
    "    if algorithm == \"SDNNR\":  # Dense NN with multi_step_window Reg.\n",
    "        print(\"Dense TD Reg.\")\n",
    "        \n",
    "        model = tfk.Sequential([\n",
    "            tfkl.Dense(units=int(.5*n_units), activation=tf.nn.leaky_relu),\n",
    "            tfkl.Dense(units=n_units, activation=tf.nn.leaky_relu),\n",
    "            tfkl.Dense(units=2*n_units, activation=tf.nn.leaky_relu,),\n",
    "            tfkl.Dense(units=4*n_units, activation=tf.nn.leaky_relu,), \n",
    "            tfkl.Dense(units=1)\n",
    "        ])\n",
    "        \n",
    "    if algorithm == \"SRNNR\":\n",
    "        print(\"Dense TD RNN Reg.\")\n",
    "        \n",
    "        model = tf.keras.models.Sequential([\n",
    "            # Shape [batch, time, features] => [batch, time, lstm_units]\n",
    "            tf.keras.layers.LSTM(n_units, return_sequences=True),\n",
    "            tf.keras.layers.LSTM(n_units, return_sequences=True),\n",
    "            # Shape => [batch, time, features]\n",
    "            tf.keras.layers.Dense(units=1)\n",
    "        ])\n",
    "        \n",
    "    \n",
    "    if algorithm == \"SCNNR\":\n",
    "        print(\"Dense TD CNN Reg.\")\n",
    "        \n",
    "        CONV_WIDTH = input_width\n",
    "        \n",
    "        model = tf.keras.Sequential([\n",
    "            tfkl.Conv1D(filters=n_units,\n",
    "                           kernel_size=(CONV_WIDTH,),\n",
    "                           activation='relu'),\n",
    "            tf.keras.layers.Dense(units=n_units, activation='relu'),\n",
    "            tf.keras.layers.Dense(units=1),\n",
    "        ])\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    window = WindowGenerator(\n",
    "        input_width=input_width, label_width=label_width, shift=shift,\n",
    "        train_df=train_df, val_df=val_df, test_df=test_df,\n",
    "        label_columns=[label_columns])\n",
    "\n",
    "    print(\"window:\", window)\n",
    "\n",
    "\n",
    "    # Training and validation of the model\n",
    "    \n",
    "    history, y_preds_model, y_train_np, x_train_np, y_test_np, x_test_np = compile_and_fit(model, window)\n",
    "    \n",
    "    \n",
    "    print(\"y_preds_model:\", y_preds_model.shape, len(y_test_np))\n",
    "    \n",
    "    # For now it is not available\n",
    "    # explainer, shap_indices, shap_name, shap_val = apply_shap_summary_plot(model=model,\n",
    "    #                                                 x_train=train_df, \n",
    "    #                                                 y_train=None, \n",
    "    #                                                 x_test=test_df, \n",
    "    #                                                 y_test=None, \n",
    "    #                                                 n_clusters=10,\n",
    "    #                                                 model_name=algorithm, \n",
    "    #                                                 data_type=data_type\n",
    "    #                                                )\n",
    "    \n",
    "    tmp = [1, 2, 3, 4, 5, 6]\n",
    "    tmp_ = [\"NaN\", \"NaN\", \"NaN\", \"NaN\", \"NaN\", \"NaN\"]\n",
    "    \n",
    "    df_cmp = final_comparison(df=df_cmp, \n",
    "                              y_preds=y_preds_model,\n",
    "                              y_trues=y_test_np, \n",
    "                              name=algorithm+'-Shap',\n",
    "                              data_name=data_type,\n",
    "                              indices=tmp,  # shap_indices,\n",
    "                              values_features_importance=[tmp],  # shap_val,\n",
    "                              name_important_features=tmp_  # shap_name\n",
    "                             )\n",
    "\n",
    "    \n",
    "    return df_cmp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "tmp_ = train_eval_a_sequential_regressor(train_df=train_df, \n",
    "                                         val_df=val_df,\n",
    "                                         test_df=test_df,\n",
    "                                         input_width=1,  # the main difference\n",
    "                                         label_width=1,  # the main difference\n",
    "                                         shift=1,\n",
    "                                         algorithm=\"Baseline\", \n",
    "                                         df_cmp=df_cmp_real_seq,\n",
    "                                         data_type=\"real-Seq\",\n",
    "                                         label_columns=\"Ic_norm\", \n",
    "                                        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "tmp_ = train_eval_a_sequential_regressor(train_df=train_df, \n",
    "                                         val_df=val_df,\n",
    "                                         test_df=test_df,\n",
    "                                         input_width=1,  # the main difference\n",
    "                                         label_width=1,  # the main difference\n",
    "                                         shift=1,\n",
    "                                         algorithm=\"SDNNR\", \n",
    "                                         df_cmp=df_cmp_real_seq,\n",
    "                                         data_type=\"real-Seq\",\n",
    "                                         label_columns=\"Ic_norm\", \n",
    "                                        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "tmp_ = train_eval_a_sequential_regressor(train_df=train_df, \n",
    "                                         val_df=val_df,\n",
    "                                         test_df=test_df,\n",
    "                                         input_width=3,  # the main difference \n",
    "                                         label_width=1,  # the main difference\n",
    "                                         shift=1,\n",
    "                                         algorithm=\"SCNNR\", \n",
    "                                         df_cmp=df_cmp_real_seq,\n",
    "                                         data_type=\"real-Seq\",\n",
    "                                         label_columns=\"Ic_norm\", \n",
    "                                        )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "tmp_ = train_eval_a_sequential_regressor(train_df=train_df, \n",
    "                                         val_df=val_df,\n",
    "                                         test_df=test_df,\n",
    "                                         input_width=24,  # the main difference\n",
    "                                         label_width=24,  # the main difference\n",
    "                                         shift=1,\n",
    "                                         algorithm=\"SRNNR\", \n",
    "                                         df_cmp=df_cmp_real_seq,\n",
    "                                         data_type=\"real-Seq\",\n",
    "                                         label_columns=\"Ic_norm\", \n",
    "                                        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion over real-only data:\n",
    "\n",
    "- All three algorithms obtained acceptable results w.r.t MAE, MRAE, RMSE.\n",
    "\n",
    "- Although all of these three also obtain acceptable r^2 score, however, RF is the winner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthetic-only data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combined data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall Conclusion:\n",
    "\n",
    "\n",
    "<!-- This study trained three regressors over a) real-data; b) synthetically generated data; c) their combination.\n",
    "\n",
    "We used four metrics to evaluate and compare the obtained results.\n",
    "\n",
    "- W.r.t MAE, MRAE, RMSE, all three algorithms, obtained excellent results over all three types of data sets.\n",
    "\n",
    "- W.r.t R^2 score, DNN-Reg obtained outstanding results over synthetic only data. It is also a close follower of the combined data set winner, which is RF. \n",
    "\n",
    "- RF wins the combined and real-only data, with relatively acceptable results.\n",
    "\n",
    "\n",
    "\n",
    "Future work: \n",
    "\n",
    "- I am going to re-train DNN-Reg with more epochs and smaller batch-size to improve its performance (hopefully). \n",
    "\n",
    " -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_windowed_dataset(X, y, time_steps):\n",
    "#     Xs, ys = [], []\n",
    "#     for i in range(len(X)-time_steps):\n",
    "#         v = X.iloc[i:(i+time_steps)].values\n",
    "#         Xs.append(v)\n",
    "#         ys.append(y.iloc[i+time_steps])\n",
    "#     return np.asarray(Xs), np.asarray(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def windowed_dataset(X, y, time_steps):\n",
    "#     ds = tfk.preprocessing.timeseries_dataset_from_array(\n",
    "#         data=X,\n",
    "#         targets=y,\n",
    "#         sequence_length=time_steps,\n",
    "#         sequence_stride=1,\n",
    "#         shuffle=False,\n",
    "#     )\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfGvenv",
   "language": "python",
   "name": "tfgvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

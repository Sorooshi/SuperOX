{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shap\n",
    "import wandb\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn import metrics\n",
    "from joblib import dump, load\n",
    "from Soroosh_utilities import *\n",
    "import tensorflow_probability as tfp\n",
    "from Soroosh_feature_importance import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "np.set_printoptions(suppress=True, precision=6)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfd = tfp.distributions\n",
    "tfpl = tfp.layers\n",
    "tfk = tf.keras\n",
    "tfkl = tf.keras.layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: GPU device not found.\n"
     ]
    }
   ],
   "source": [
    "if tf.test.gpu_device_name() != '/device:GPU:0':\n",
    "    print('WARNING: GPU device not found.')\n",
    "else:\n",
    "    print('SUCCESS: Found GPU: {}'.format(tf.test.gpu_device_name()))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The purposes of this set of studies:\n",
    "\n",
    "- We intent to predict the critical current, determine the important features, and visualize them.\n",
    "\n",
    "- Previously, it has been done for real-world data. In this set of studies, not only will we repeat those experiments, but also we use the synthetically generated data to increase our test samples. To this end, we pursue the following framework.\n",
    "\n",
    "    - 1) Train regressors on only real-world data;\n",
    "    - 2) Train regressors on only synthetic data;\n",
    "    - 3) Train regressors on combination of both (50%-50%).\n",
    "\n",
    "\n",
    "- To this end we use, three regression algorithms: a)RF, b) GBR-LS, c) DNNR.\n",
    "\n",
    "- Once the training is completed we will apply: 1) Gini-index, 2) Permutation, to determine the importance of features. Finally we use \"Partial dependence\" to visualize the impact of important features on the target values.\n",
    "\n",
    "\n",
    "- Note: For more see the notion page below\n",
    "\n",
    "https://www.notion.so/SuperOx-936b1b2ce7b14f20bd76578c82305e2b\n",
    "\n",
    "\n",
    "Note: we studied and tuned the parameters in the previous jupyter notebook. Thus we will use them as the default and given here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature importance:\n",
    "\n",
    "There exist several methods to determine the importance of features as follows.\n",
    "\n",
    "1. **The coefficients of linear regression:**\n",
    "\n",
    "    - Once the LR model is fitted, the weights can be used to interpret the features' importance.\n",
    "\n",
    "    - cons: over-simplistic.\n",
    "\n",
    "\n",
    "2. **Gini importance(or mean decrease in impurity) mechanism:** \n",
    "\n",
    "    - Gini importance is calculated as the decrease in node impurity weighted by the probability of reaching that node. The node probability can be calculated by the number of samples that reach the node, divided by the total number of samples. The higher the value, the more important the feature. \n",
    "    \n",
    "    In other words, the mean decrease in impurity importance of a feature is computed by measuring how effective the feature is at reducing uncertainty (classifiers) or variance (regressors) when creating decision trees within RFs.\n",
    "\n",
    "    - Cons: impurity-based feature importances can be misleading for high cardinality features. (It tends to inflate the importance of continuous or high-cardinality categorical variables). \n",
    "    \n",
    "\n",
    "3. **Permutation importance mechanism:** First, a baseline metric, defined by scoring, is evaluated on a (potentially different) dataset defined by the X. Next, a feature column from the validation set is permuted, and the metric is re-evaluated. The permutation importance is defined to be the difference between the baseline metric and metric from permutating the feature column.\n",
    "\n",
    "    - Cons: computationally, more expensive than Gini importance, but still more reliable. Moreover, highly correlated features could yield biased estimations or overestimations of the importance of features (Strobl et al. (2007), Nicodemus et al. (2010)).\n",
    "\n",
    "\n",
    "### Additive Feature Attribution Methods: (SHAP lib)\n",
    "\n",
    "The best explanation of a simple model is the model itself; it perfectly represents itself and is easy to understand. For complex models, such as ensemble methods or deep networks, we cannot use the original model as its own best explanation because it is not easy to understand. Instead, we must use a simpler explanation model, which we define as any interpretable approximation of the original model. We show below that six current explanation methods from the literature all use the same explanation model. This previously unappreciated unity has interesting implications, which we describe in later sections.\n",
    "\n",
    "Let $f$ be the original prediction model to be explained and $g$ the explanation model. \n",
    "Here, we focus on local methods designed to explain a prediction $f(x)$ based on a single input $x$. Explanation models often use simplified inputs $x′$ that map to the original inputs through a mapping function $x = h_x(x′)$. Local methods try to ensure $g(z′) ≈ f(h_x(z′))$ whenever $z′ ≈ x′$. (Note that $h_x(x′) = x$ even though $x′$ may contain less information than $x$ because $h_x$ is specific to the current input $x$.)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Definition.**\n",
    "\n",
    "Additive feature attribution methods have an explanation model that is a linear function of binary variables:\n",
    "\n",
    "$g(z^′)=φ_0 + \\sum_{i=1}^{M}φ_iz_i^{′} \\ \\ \\ Eqn.(1) $ \n",
    "\n",
    "\n",
    "where $z′ \\in \\{0, 1\\}^M$ , $M$ is the number of simplified input features, and $φ_i \\mathbb{R}$.\n",
    "\n",
    "\n",
    "\n",
    "4. **LIME:**\n",
    "\n",
    "The LIME method interprets individual model predictions based on locally approximating the model around a given prediction. \n",
    "The local linear explanation model that LIME uses adheres to Equation 1 exactly and is thus an additive feature attribution method.\n",
    "LIME refers to simplified inputs $x^′$ as “interpretable inputs,” and the mapping $x = h_x(x^′)$ converts a binary vector of interpretable inputs into the original input space. Different types of $h_x$ mappings are used for different input spaces. For bag of words text features, $h_x$ converts a vector of 1’s or 0’s (present or not) into the original word count if the simplified input is one, or zero if the simplified input is zero. For images, $h_x$ treats the image as a set of super pixels; it then maps 1 to leaving the super pixel as its original value and 0 to replacing the super pixel with an average of neighboring pixels (this is meant to represent being missing).\n",
    "\n",
    "\n",
    "\n",
    "5. **DeepLIFT:**\n",
    "\n",
    "DeepLIFT attributes to each input $x_i$ a value $C_{∆xi ∆y}$ that represents the effect of that input being set to a reference value as opposed to its original value. This means that for DeepLIFT, the mapping $x = h_x(x′)$ converts binary values into the original inputs, where 1 indicates that an input takes its original value, and 0 indicates that it takes the reference value. The reference value, though chosen by the user, represents a typical uninformative background value for the feature.\n",
    "DeepLIFT uses a \"summation-to-delta\" property that states:  $ \\sum_{i=1}^{n} C_{∆xi∆o} =∆o,$ \n",
    "\n",
    "\n",
    "where $o = f(x)$ is the model output, $∆_o = f(x) − f(r)$, $∆x_i = x_i − r_i$, and r is the reference input. If we let $φ_i = C_{∆xi∆o}$ and $φ_0 = f(r)$, then DeepLIFT’s explanation model matches Equation 1 and is thus another additive feature attribution method.\n",
    "\n",
    "\n",
    "6. **Layer-Wise Relevance Propagation:**\n",
    "\n",
    "The layer-wise relevance propagation method interprets the predictions of deep networks [1]. As noted by Shrikumar et al., this menthod is equivalent to DeepLIFT with the reference activations of all neurons fixed to zero. Thus, x = hx(x′) converts binary values into the original input space, where 1 means that an input takes its original value, and 0 means an input takes the 0 value. Layer-wise relevance propagation’s explanation model, like DeepLIFT’s, matches Equation 1.\n",
    "\n",
    "\n",
    "7. **Shapley regression values** \n",
    "are feature importances for linear models in the presence of multicollinearity. This method requires retraining the model on all feature subsets $S ⊆ F$ , where $F$ is the set of all features. It assigns an importance value to each feature that represents the effect on the model prediction of including that feature. To compute this effect, a model $f_{S∪\\{i\\}}$ is trained with that feature present, and another model $fS$ is trained with the feature withheld. Then, predictions from the two models are compared on the current input $f_{S∪\\{i\\}}(x_{S∪\\{i\\}}) − f_S(x_S)$, where $x_S$ represents the values of the input features in the set $S$. Since the effect of withholding a feature depends on other features in the model, the preceding differences are computed for all possible subsets $S ⊆ F \\ \\{i\\}$. The Shapley values are then computed and used as feature attributions. They are a weighted average of all possible differences (Eqn.(4) from [1]).\n",
    "\n",
    "\n",
    "8. **Shapley sampling values**\n",
    "are meant to explain any model by: (1) applying sampling approximations to Equation 4, and (2) approximating the effect of removing a variable from the model by integrating over samples from the training dataset. This eliminates the need to retrain the model and allows fewer than $2^{|F |}$ differences to be computed. Since the explanation model form of Shapley sampling values is the same as that for Shapley regression values, it is also an additive feature attribution method.\n",
    "\n",
    "9. **Quantitative input influence**\n",
    "is a broader framework that addresses more than feature attributions. However, as part of its method it independently proposes a sampling approximation to Shapley values that is nearly identical to Shapley sampling values. It is thus another additive feature attribution method.\n",
    "\n",
    "\n",
    "**SHAP (SHapley Additive exPlanation) Values**: in [1] it was shown that indeed, 4,5, 6 corresponds to 7,8, 9 in respect. Thus the authors combined them together and proposed a library called \"shap\" to this end, with set of handy tools which we will use in this study.\n",
    "\n",
    "10. **SHAP Feature Importance:** \n",
    "\n",
    "The idea behind SHAP feature importance is simple: Features with large absolute Shapley values are important. Since we want the global importance, we average the absolute Shapley values per feature across the data:\n",
    "\n",
    "$$\n",
    "I_j = \\sum_{j=1}^{n} |\\phi_j^{i}|\n",
    "$$\n",
    "\n",
    "\n",
    "Next, we sort the features by decreasing importance and plot them.\n",
    "\n",
    "### Conclusion over feature importance methods:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "[1]: Lundberg, S. and Lee, S.I., 2017. A unified approach to interpreting model predictions. arXiv preprint arXiv:1705.07874.\n",
    "\n",
    "for high-level description, see https://towardsdatascience.com/one-feature-attribution-method-to-supposedly-rule-them-all-shapley-values-f3e04534983d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializatin for comparison of methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_algs = ['RF-Gini', 'RF-Perm', 'RF-Shap', 'GBR-Gini', 'GBR-Perm', 'GBR-Shap',\n",
    "            'DNNR-Shap']  # Regression algorithms ('Bayes_Reg')\n",
    "results = ['MAE', 'MRAE', 'RMSE', 'R^2-Score', \n",
    "           'Value, Name-1', 'Value, Name-2', 'Value, Name-3', \n",
    "           'Value, Name-4', 'Value, Name-5']  \n",
    "df_cmp_real = pd.DataFrame(index=reg_algs, columns=results) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_algs = [\"Baseline-Shap\", \"SDNNR-Shap\", \n",
    "            \"SCNNR-Shap\", \"SRNNR-Shap\", ]  # Regression algorithms ('Bayes_Reg')\n",
    "results = ['MAE', 'MRAE', 'RMSE', 'R^2-Score', \n",
    "           'Value, Name-1', 'Value, Name-2', 'Value, Name-3', \n",
    "           'Value, Name-4', 'Value, Name-5']  \n",
    "df_cmp_real_seq = pd.DataFrame(index=reg_algs, columns=results) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_algs = ['RF-Gini', 'RF-Perm', 'GBR-Gini', 'GBR-Perm',\n",
    "            'DNNR-Gini', 'DNNR-Perm']  # Regression algorithms ('Bayes_Reg')\n",
    "\n",
    "results = ['MAE', 'MRAE', 'RMSE', 'R^2-Score', \n",
    "           'Value, Name-1', 'Value, Name-2', 'Value, Name-3', \n",
    "           'Value, Name-4', 'Value, Name-5']  \n",
    "\n",
    "df_cmp_synthetic = pd.DataFrame(index=reg_algs, columns=results) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_algs = ['RF-Gini', 'RF-Perm', 'GBR-Gini', 'GBR-Perm',\n",
    "            'DNNR-Gini', 'DNNR-Perm']  # Regression algorithms ('Bayes_Reg')\n",
    "results = ['MAE', 'MRAE', 'RMSE', 'R^2-Score', \n",
    "           'Value, Name-1', 'Value, Name-2', 'Value, Name-3', \n",
    "           'Value, Name-4', 'Value, Name-5']\n",
    "\n",
    "df_cmp_combined = pd.DataFrame(index=reg_algs, columns=results) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset and initialization of experiment settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Non-sequential\n",
    "project = \"SuperOX\"\n",
    "data_type = \"real-reduced_features\"\n",
    "run_id = \"increasing the window size for reduced-features -input_width=100, label_width=1, shift=1, winsize=101\"  # \n",
    "\n",
    "# Sequential\n",
    "project_ = project\n",
    "\n",
    "run_id_ = run_id\n",
    "\n",
    "data_type_ = data_type\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pld_complete = catalog.load('pld_complete').dropna().sort_values('pos', ascending=True)\n",
    "# pld_complete_zscore = pd.read_csv(\"/home/soroosh/SearchOX/data/pld_complete_zscore.csv\", index_col=False)\n",
    "\n",
    "if data_type == \"real-all\" or data_type == \"real-pilot\":\n",
    "    pld_complete_range = pd.read_csv(\"/home/soroosh/Desktop/SearchOX/data/pld_complete_range.csv\",\n",
    "                                     index_col=False)\n",
    "\n",
    "elif data_type == \"real-reduced_features\":\n",
    "    pld_complete_range = pd.read_csv(\"/home/soroosh/Desktop/SearchOX/data/pld_complete_range_reduced.csv\",\n",
    "                                     index_col=False)\n",
    "    \n",
    "elif data_type == \"synthetic-all\":\n",
    "    pld_complete_range_synthetic = np.loadtxt(\"/home/soroosh/Desktop/SearchOX/data/x_r_synthetic.npy\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>median_Voltage_HSR_V_1025</th>\n",
       "      <th>median_Voltage_HSR_V_1027</th>\n",
       "      <th>median_Voltage_HSR_V_1030</th>\n",
       "      <th>median_Voltage_HSL_V_1025</th>\n",
       "      <th>median_Voltage_HSL_V_1027</th>\n",
       "      <th>median_Voltage_HSL_V_1030</th>\n",
       "      <th>median_Voltage_HF_V_1025</th>\n",
       "      <th>median_Voltage_HF_V_1030</th>\n",
       "      <th>median_Voltage_HC_V_1025</th>\n",
       "      <th>median_Voltage_HC_V_1030</th>\n",
       "      <th>...</th>\n",
       "      <th>std_HV_1025</th>\n",
       "      <th>std_TubeTemp_1025</th>\n",
       "      <th>std_Egy_1025</th>\n",
       "      <th>std_Pressure_1025</th>\n",
       "      <th>std_Sigma_1025</th>\n",
       "      <th>Speed</th>\n",
       "      <th>X FWHM</th>\n",
       "      <th>Coolness</th>\n",
       "      <th>Ic</th>\n",
       "      <th>Ic_norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.129479</td>\n",
       "      <td>0.177414</td>\n",
       "      <td>0.005142</td>\n",
       "      <td>0.69562</td>\n",
       "      <td>-0.256257</td>\n",
       "      <td>-0.015406</td>\n",
       "      <td>-0.009215</td>\n",
       "      <td>-0.021102</td>\n",
       "      <td>0.057123</td>\n",
       "      <td>0.038624</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.520227</td>\n",
       "      <td>-0.331198</td>\n",
       "      <td>0.074951</td>\n",
       "      <td>-0.154349</td>\n",
       "      <td>-0.269119</td>\n",
       "      <td>0.009841</td>\n",
       "      <td>0.039589</td>\n",
       "      <td>0.053006</td>\n",
       "      <td>496.2</td>\n",
       "      <td>1.767913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.110059</td>\n",
       "      <td>0.340245</td>\n",
       "      <td>0.005142</td>\n",
       "      <td>0.69562</td>\n",
       "      <td>-0.321465</td>\n",
       "      <td>-0.015406</td>\n",
       "      <td>-0.009215</td>\n",
       "      <td>-0.021102</td>\n",
       "      <td>0.050178</td>\n",
       "      <td>0.038624</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.520227</td>\n",
       "      <td>-0.331198</td>\n",
       "      <td>0.057016</td>\n",
       "      <td>-0.149436</td>\n",
       "      <td>-0.280188</td>\n",
       "      <td>-0.005266</td>\n",
       "      <td>0.039589</td>\n",
       "      <td>0.053006</td>\n",
       "      <td>494.7</td>\n",
       "      <td>1.762568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.119769</td>\n",
       "      <td>0.340245</td>\n",
       "      <td>0.005142</td>\n",
       "      <td>0.69562</td>\n",
       "      <td>-0.321465</td>\n",
       "      <td>-0.015406</td>\n",
       "      <td>-0.007363</td>\n",
       "      <td>-0.021102</td>\n",
       "      <td>0.057123</td>\n",
       "      <td>0.038624</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.520227</td>\n",
       "      <td>-0.331198</td>\n",
       "      <td>0.019830</td>\n",
       "      <td>-0.150641</td>\n",
       "      <td>-0.282470</td>\n",
       "      <td>-0.007197</td>\n",
       "      <td>0.039589</td>\n",
       "      <td>0.053006</td>\n",
       "      <td>494.2</td>\n",
       "      <td>1.760787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.110059</td>\n",
       "      <td>0.348139</td>\n",
       "      <td>0.005142</td>\n",
       "      <td>0.69562</td>\n",
       "      <td>-0.322354</td>\n",
       "      <td>-0.015406</td>\n",
       "      <td>-0.009215</td>\n",
       "      <td>-0.021102</td>\n",
       "      <td>0.057123</td>\n",
       "      <td>0.038624</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.520227</td>\n",
       "      <td>-0.331198</td>\n",
       "      <td>0.031812</td>\n",
       "      <td>-0.146725</td>\n",
       "      <td>-0.279809</td>\n",
       "      <td>-0.010946</td>\n",
       "      <td>0.039589</td>\n",
       "      <td>0.053006</td>\n",
       "      <td>495.5</td>\n",
       "      <td>1.765419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.110059</td>\n",
       "      <td>0.264593</td>\n",
       "      <td>0.005142</td>\n",
       "      <td>0.69562</td>\n",
       "      <td>-0.305820</td>\n",
       "      <td>-0.015406</td>\n",
       "      <td>-0.009215</td>\n",
       "      <td>-0.021102</td>\n",
       "      <td>0.057123</td>\n",
       "      <td>0.038624</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.520227</td>\n",
       "      <td>-0.331198</td>\n",
       "      <td>0.004482</td>\n",
       "      <td>-0.139048</td>\n",
       "      <td>-0.284882</td>\n",
       "      <td>-0.017147</td>\n",
       "      <td>0.042682</td>\n",
       "      <td>0.051965</td>\n",
       "      <td>497.9</td>\n",
       "      <td>1.773168</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 164 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   median_Voltage_HSR_V_1025  median_Voltage_HSR_V_1027  \\\n",
       "0                   0.129479                   0.177414   \n",
       "1                   0.110059                   0.340245   \n",
       "2                   0.119769                   0.340245   \n",
       "3                   0.110059                   0.348139   \n",
       "4                   0.110059                   0.264593   \n",
       "\n",
       "   median_Voltage_HSR_V_1030  median_Voltage_HSL_V_1025  \\\n",
       "0                   0.005142                    0.69562   \n",
       "1                   0.005142                    0.69562   \n",
       "2                   0.005142                    0.69562   \n",
       "3                   0.005142                    0.69562   \n",
       "4                   0.005142                    0.69562   \n",
       "\n",
       "   median_Voltage_HSL_V_1027  median_Voltage_HSL_V_1030  \\\n",
       "0                  -0.256257                  -0.015406   \n",
       "1                  -0.321465                  -0.015406   \n",
       "2                  -0.321465                  -0.015406   \n",
       "3                  -0.322354                  -0.015406   \n",
       "4                  -0.305820                  -0.015406   \n",
       "\n",
       "   median_Voltage_HF_V_1025  median_Voltage_HF_V_1030  \\\n",
       "0                 -0.009215                 -0.021102   \n",
       "1                 -0.009215                 -0.021102   \n",
       "2                 -0.007363                 -0.021102   \n",
       "3                 -0.009215                 -0.021102   \n",
       "4                 -0.009215                 -0.021102   \n",
       "\n",
       "   median_Voltage_HC_V_1025  median_Voltage_HC_V_1030  ...  std_HV_1025  \\\n",
       "0                  0.057123                  0.038624  ...    -0.520227   \n",
       "1                  0.050178                  0.038624  ...    -0.520227   \n",
       "2                  0.057123                  0.038624  ...    -0.520227   \n",
       "3                  0.057123                  0.038624  ...    -0.520227   \n",
       "4                  0.057123                  0.038624  ...    -0.520227   \n",
       "\n",
       "   std_TubeTemp_1025  std_Egy_1025  std_Pressure_1025  std_Sigma_1025  \\\n",
       "0          -0.331198      0.074951          -0.154349       -0.269119   \n",
       "1          -0.331198      0.057016          -0.149436       -0.280188   \n",
       "2          -0.331198      0.019830          -0.150641       -0.282470   \n",
       "3          -0.331198      0.031812          -0.146725       -0.279809   \n",
       "4          -0.331198      0.004482          -0.139048       -0.284882   \n",
       "\n",
       "      Speed    X FWHM  Coolness     Ic   Ic_norm  \n",
       "0  0.009841  0.039589  0.053006  496.2  1.767913  \n",
       "1 -0.005266  0.039589  0.053006  494.7  1.762568  \n",
       "2 -0.007197  0.039589  0.053006  494.2  1.760787  \n",
       "3 -0.010946  0.039589  0.053006  495.5  1.765419  \n",
       "4 -0.017147  0.042682  0.051965  497.9  1.773168  \n",
       "\n",
       "[5 rows x 164 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pld_complete_range.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## splitting the sequential data\n",
    "\n",
    "### Real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((18561, 162), (18561,), (18561,))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_r_real = pld_complete_range.loc[:,\n",
    "                                  ~pld_complete_range.columns.isin(['Ic', 'Ic_norm', 'pos'])]  # .to_numpy()\n",
    "\n",
    "y_ic_real = pld_complete_range['Ic']  # .to_numpy()\n",
    "y_ic_norm_real = pld_complete_range['Ic_norm']  # .to_numpy()\n",
    "# pos_real = pld_complete_range['pos']  # .to_numpy()\n",
    "# pos_real.shape\n",
    "\n",
    "x_r_real.shape, y_ic_real.shape, y_ic_norm_real.shape, \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(162, 163)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pos_idx = pld_complete_range.columns.get_loc(\"pos\")\n",
    "ic_idx = pld_complete_range.columns.get_loc(\"Ic\")\n",
    "ic_norm_idx = pld_complete_range.columns.get_loc(\"Ic_norm\")\n",
    "# pos_idx, \n",
    "ic_idx, ic_norm_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 2000  # previously, it was 500\n",
    "learning_rate = 1e-5  # [1e-2, 1e-3, 1e-5] \n",
    "batch_size = 64  # [32, 64, 256]\n",
    "n_units = 128\n",
    "\n",
    "input_shape = (x_r_real.shape[1])  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save synthetic data as df and then modify the two (four) cell below "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_r_synthetic_1 = pld_complete_range_synthetic[:, : pos_idx]\n",
    "# x_r_synthetic_2 = pld_complete_range_synthetic[:, pos_idx+1:ic_idx]\n",
    "\n",
    "\n",
    "# x_r_synthetic = np.concatenate([x_r_synthetic_1, x_r_synthetic_2], axis=1)\n",
    "# x_r_synthetic_1.shape, x_r_synthetic_2.shape, x_r_synthetic.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_ic_synthetic = pld_complete_range_synthetic[:, ic_idx]\n",
    "# y_ic_norm_synthetic = pld_complete_range_synthetic[:, ic_norm_idx] \n",
    "# pos_synthetic = pld_complete_range_synthetic[:, pos_idx]\n",
    "\n",
    "# x_r_synthetic.shape, y_ic_synthetic.shape, y_ic_norm_synthetic.shape, pos_synthetic.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_r_combined = np.concatenate([x_r_real, x_r_synthetic], axis=0)\n",
    "# y_ic_combined = np.concatenate([y_ic_real, y_ic_synthetic], axis=0)\n",
    "# y_ic_norm_combined = np.concatenate([y_ic_norm_real, y_ic_norm_synthetic], axis=0)\n",
    "\n",
    "# x_r_combined.shape, y_ic_combined.shape, y_ic_norm_combined.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert not np.any(np.isnan(x_r_real))\n",
    "# assert not x_r_real.shape != x_r_synthetic.shape\n",
    "# assert not x_r_combined.shape[0] != int(2*x_r_synthetic.shape[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## splitting the non-sequential data\n",
    "\n",
    "### Real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((11136, 162), (3713, 162), (3712, 162), (11136,), (3713,), (3712,))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_r_train_real, x_r_test_real, \\\n",
    "y_train_real, y_test_real = train_test_split(x_r_real,\n",
    "                                             y_ic_norm_real,\n",
    "                                             test_size=0.40,\n",
    "                                             random_state=43,)\n",
    "\n",
    "x_r_val_real, x_r_test_real, \\\n",
    "y_val_real, y_test_real = train_test_split(x_r_test_real,\n",
    "                                           y_test_real,\n",
    "                                           test_size=0.5,\n",
    "                                           random_state=43,)\n",
    "\n",
    "\n",
    "x_r_train_real.shape, x_r_test_real.shape, \\\n",
    "x_r_val_real.shape, y_train_real.shape, \\\n",
    "y_test_real.shape, y_val_real.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((11136, 163), (3713, 163), (3712, 163), (11136,), (3713, 163), (3712,))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pld_complete_range_ = pld_complete_range.loc[:,\n",
    "                                             ~pld_complete_range.columns.isin(['Ic', 'pos'])]\n",
    "\n",
    "column_indices = {name: i for i, name in enumerate(pld_complete_range_.columns)}\n",
    "\n",
    "n = len(x_r_real)\n",
    "x_r_train_real_seq = pld_complete_range_[0:int(n*0.6)]\n",
    "y_train_real_seq = y_ic_norm_real[0:int(n*0.6)]\n",
    "\n",
    "x_r_val_real_seq = pld_complete_range_[int(n*0.6):int(n*0.8)]\n",
    "y_val_real_seq = y_ic_norm_real[int(n*0.6):int(n*0.8)]\n",
    "\n",
    "\n",
    "x_r_test_real_seq = pld_complete_range_[int(n*0.8):]\n",
    "y_test_real_seq = pld_complete_range_[int(n*0.8):]\n",
    "\n",
    "\n",
    "\n",
    "x_r_train_real_seq.shape, x_r_test_real_seq.shape, \\\n",
    "x_r_val_real_seq.shape, y_train_real_seq.shape, \\\n",
    "y_test_real_seq.shape, y_val_real_seq.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_r_train_synthetic, x_r_test_synthetic,\\\n",
    "# y_train_synthetic, y_test_synthetic = train_test_split(x_r_synthetic,\n",
    "#                                                        y_ic_norm_synthetic,\n",
    "#                                                        test_size=0.40,\n",
    "#                                                        random_state=43,)\n",
    "\n",
    "# x_r_val_synthetic, x_r_test_synthetic,\\\n",
    "# y_val_synthetic, y_test_synthetic = train_test_split(x_r_test_synthetic,\n",
    "#                                                      y_test_synthetic,\n",
    "#                                                      test_size=0.5,\n",
    "#                                                      random_state=43,)\n",
    "\n",
    "\n",
    "# x_r_train_synthetic.shape, x_r_test_synthetic.shape, \\\n",
    "# x_r_val_synthetic.shape, y_train_synthetic.shape,  y_test_synthetic.shape, y_val_synthetic.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combined data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_r_train_combined, x_r_test_combined, \\\n",
    "# y_train_combined, y_test_combined = train_test_split(x_r_combined,\n",
    "#                                                      y_ic_norm_combined,\n",
    "#                                                      test_size=0.40,\n",
    "#                                                      random_state=43,)\n",
    "\n",
    "# x_r_val_combined, x_r_test_combined, \\\n",
    "# y_val_combined, y_test_combined = train_test_split(x_r_test_combined,\n",
    "#                                                    y_test_combined,\n",
    "#                                                    test_size=0.5,\n",
    "#                                                    random_state=43,)\n",
    "\n",
    "\n",
    "# x_r_train_combined.shape, x_r_test_combined.shape, \\\n",
    "# x_r_val_combined.shape, y_train_combined.shape, \\\n",
    "# y_test_combined.shape, y_val_combined.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determining the importance of features\n",
    "A) Permutation\n",
    "B) Gini-index\n",
    "C)Shap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training RF\n",
    "\n",
    "- For more about tuning the parameters see the u01-*.ipynb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_eval_rf(x_train, y_train, x_test, y_test, name):\n",
    "    \n",
    "    rf_reg = RandomForestRegressor(n_estimators=100, \n",
    "                                   n_jobs=-2, \n",
    "                                   criterion='mse', \n",
    "                                   min_samples_leaf=1,\n",
    "                                   verbose=1)\n",
    "    \n",
    "    rf_reg.fit(x_train, y_train)\n",
    "    \n",
    "    y_preds_rf = rf_reg.predict(x_test)\n",
    "    \n",
    "    filename = \"rf_reg-\" + name +  \"-.joblib\"\n",
    "    dump(rf_reg, \"../saved_model/\"+ filename )\n",
    "    \n",
    "    features_importance_gini = rf_reg.feature_importances_\n",
    "    \n",
    "    features_importance_perm = permutation_importance(estimator=rf_reg,\n",
    "                                                      X=x_test, y=y_test,\n",
    "                                                      n_repeats=5,\n",
    "                                                      n_jobs=-2,\n",
    "                                                      random_state=0)\n",
    "    \n",
    "    explainer, features_importance_shap = apply_shap_summary_plot(model=rf_reg,\n",
    "                                                                  x_train=x_train, y_train=y_train, \n",
    "                                                                  x_test=x_test, y_test=y_test, \n",
    "                                                                  n_clusters=10,\n",
    "                                                                  model_name=\"RF\", data_type=name\n",
    "                                                                 )\n",
    "    \n",
    "    \n",
    "    return y_preds_rf, features_importance_gini, features_importance_perm, features_importance_shap, rf_reg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Gradient Boosting Regressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_eval_gbr(x_train, y_train, x_test, y_test, name):\n",
    "    \n",
    "    gbr_ls = GradientBoostingRegressor(loss='ls', verbose=1,)\n",
    "    gbr_ls.fit(x_train, y_train)\n",
    "    y_preds_gbr_ls = gbr_ls.predict(x_test)\n",
    "    \n",
    "    features_importance_gini = gbr_ls.feature_importances_\n",
    "    \n",
    "    features_importance_perm = permutation_importance(estimator=gbr_ls,\n",
    "                                                     X=x_test, y=y_test,\n",
    "                                                     n_repeats=5,\n",
    "                                                     n_jobs=-2,\n",
    "                                                     random_state=0)\n",
    "    \n",
    "    filename = \"gbr_ls-\" + name +  \"-.joblib\"\n",
    "    dump(gbr_ls, \"../saved_model/\"+ filename )\n",
    "        \n",
    "    \n",
    "    explainer, features_importance_shap = apply_shap_summary_plot(model=gbr_ls,\n",
    "                                                      x_train=x_train, y_train=y_train, \n",
    "                                                      x_test=x_test, y_test=y_test, \n",
    "                                                      n_clusters=10,\n",
    "                                                      model_name=\"GBR\", data_type=name\n",
    "                                                     )\n",
    "    \n",
    "#     features_importance_shap = shap_importance(shap_values, x_test)\n",
    "\n",
    "    \n",
    "    return y_preds_gbr_ls, features_importance_gini, features_importance_perm, features_importance_shap, gbr_ls\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DNN-Regressor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DnnReg(tfk.Model):\n",
    "    \n",
    "    def __init__(self, n_units, n_features, name='dnn_reg', **kwargs):\n",
    "        super(DnnReg, self).__init__(name=name, **kwargs)\n",
    "        self.n_units = n_units\n",
    "        self.n_features = n_features\n",
    "        \n",
    "        self.input_layer = tfkl.InputLayer(input_shape=self.n_features)\n",
    "        self.cast_layer = tfkl.Lambda(lambda x: tf.cast(x, tf.float32))\n",
    "        self.dense_1 = tfkl.Dense(units=int(.5*self.n_units), activation=tf.nn.leaky_relu)\n",
    "        self.dense_2 = tfkl.Dense(units=self.n_units, activation=tf.nn.leaky_relu)\n",
    "        self.dense_3 = tfkl.Dense(units=2*self.n_units, activation=tf.nn.leaky_relu,)\n",
    "        self.dense_4 = tfkl.Dense(units=4*self.n_units, activation=tf.nn.leaky_relu,)\n",
    "        self.regressor = tfkl.Dense(units=1)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = self.input_layer(inputs)\n",
    "        x = self.cast_layer(x)\n",
    "        x = self.dense_1(x)\n",
    "        x = self.dense_2(x)\n",
    "        x = self.dense_3(x)\n",
    "        x = self.dense_4(x)\n",
    "        regression = self.regressor(x)\n",
    "        return regression\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_eval_dnn_reg(dnn_reg, x_train, y_train, x_val, y_val, x_test, y_test, name):\n",
    "\n",
    "    callback = tfk.callbacks.EarlyStopping(monitor='loss', patience=5)  # for early-stop\n",
    "\n",
    "    dnn_reg.compile(optimizer=tf.optimizers.Adam(learning_rate=learning_rate),\n",
    "                    loss='mse',\n",
    "                   )\n",
    "\n",
    "    history_dnn_ref = dnn_reg.fit(x=x_train, y=y_train,\n",
    "                                  epochs=n_epochs, batch_size=batch_size,\n",
    "                                  validation_data=(x_val, y_val),\n",
    "                                  # callbacks=[callback],\n",
    "                                 )\n",
    "\n",
    "\n",
    "    filename = \"dnn-reg-\" + name \n",
    "    dnn_reg.save_weights('../saved_model/' + filename + '.h5')\n",
    "\n",
    "    y_preds_dnn_reg = dnn_reg.predict(x_test)\n",
    "    \n",
    "    \n",
    "    explainer, features_importance_shap = apply_shap_summary_plot(model=dnn_reg,\n",
    "                                                     x_train=x_train, \n",
    "                                                     y_train=y_train, \n",
    "                                                     x_test=x_test, \n",
    "                                                     y_test=y_test, \n",
    "                                                     n_clusters=10,\n",
    "                                                     model_name=\"DNNR\", \n",
    "                                                     data_type=name\n",
    "                                                    )\n",
    "    \n",
    "    \n",
    "    print(\"y_preds_dnn_reg:\", y_preds_dnn_reg.shape)\n",
    "    \n",
    "    return y_preds_dnn_reg, features_importance_shap, dnn_reg\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real-only data \n",
    "\n",
    "## RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# rnd_sample = np.random.randint(0, 10000, size=(1000))\n",
    "\n",
    "# x_r_train_real = x_r_train_real.iloc[rnd_sample, 370:]\n",
    "# y_train_real = y_train_real.iloc[rnd_sample]\n",
    "\n",
    "# rnd_sample = np.random.randint(0, 3000, size=(500))\n",
    "# x_r_val_real = x_r_val_real.iloc[rnd_sample, 370:]\n",
    "# y_val_real = y_val_real.iloc[rnd_sample]\n",
    "\n",
    "# rnd_sample = np.random.randint(0, 3000, size=(500))\n",
    "# x_r_test_real = x_r_test_real.iloc[rnd_sample, 370: ]\n",
    "# y_test_real = y_test_real.iloc[rnd_sample]\n",
    "\n",
    "\n",
    "# print(x_r_train_real.shape, x_r_val_real.shape, x_r_test_real.shape, \n",
    "#       y_train_real.shape, y_val_real.shape, y_test_real.shape, )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# rnd_sample = np.random.choice(train_df_real.index, 100)\n",
    "\n",
    "\n",
    "\n",
    "# train_df = pld_complete_range_.iloc[:1000, 160:]\n",
    "# val_df = pld_complete_range_.iloc[1000:1500, 160:]\n",
    "# test_df = pld_complete_range_.iloc[1500:2000, 160:]\n",
    "\n",
    "\n",
    "# x_r_train_real_seq = x_r_train_real_seq.iloc[:100, 160:]\n",
    "# y_train_real_seq = y_train_real_seq.iloc[:100, ]\n",
    "# x_r_val_real_seq = x_r_val_real_seq.iloc[:100, 160:]\n",
    "# y_val_real_seq = y_val_real_seq.iloc[:100,]\n",
    "# x_r_test_real_seq = x_r_test_real_seq.iloc[:100, 160:]\n",
    "# y_test_real_seq = y_test_real_seq.iloc[:100, ]\n",
    "\n",
    "# train_df.shape, test_df.shape, val_df.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WandB "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_a_wandb(name, project, notes, group):\n",
    "    \n",
    "    \"\"\" name := the within the project name, e.g., RF-reg-1\n",
    "        project := the project name, e.g., Non-sequential Regressions \n",
    "        notes := Description, e.g., Non-sequential Regressions Comparison for SuperOX\n",
    "        group := name of experiment or the algorithm under consideration, e.g., RF-1\n",
    "    \"\"\"\n",
    "    \n",
    "    run = wandb.init(name=name, \n",
    "                     project=project, \n",
    "                     notes = notes,\n",
    "                     entity='sorooshi',\n",
    "                     group=group, \n",
    "#                      tags=[\"metric\", \"Gini\", \"Perm.\", \"Shap\"]\n",
    "                    )\n",
    "    \n",
    "    return run\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wandb_metrics(run, y_trues, y_preds):\n",
    "    \n",
    "    run.log({\n",
    "        \"MAE\" : mae(y_trues=y_trues, y_preds=y_preds),\n",
    "        \"MRAE\" : mrae(y_trues=y_trues, y_preds=y_preds),\n",
    "        \"RMSE\" : rmse(y_trues=y_trues, y_preds=y_preds),\n",
    "        \"R^2-Score\": metrics.r2_score(y_trues, y_preds)\n",
    "    })\n",
    "    \n",
    "    return run\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wandb_features_importance(run, values_features_importance, \n",
    "                              name_important_features, \n",
    "                              indices_important_features,\n",
    "                              importance_method):\n",
    "\n",
    "    counter = 0\n",
    "    for i in range(len(indices_important_features)):\n",
    "        if counter < 5:\n",
    "            run.log({\n",
    "                importance_method + \n",
    "                \"-\" + name_important_features[indices_important_features[i]] +\n",
    "                \"-\" + str(i+1):  values_features_importance[0][indices_important_features[i]],\n",
    "            })\n",
    "            counter += 1\n",
    "    \n",
    "    return run\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wandb_plot_predictions(run, algorithm, y_trues, y_preds):\n",
    "    t = np.arange(len(y_trues))\n",
    "    fig, ax = plt.subplots(1, figsize=(12, 8))\n",
    "    ax.plot(t, y_trues, lw=1.5, c='g', label=\"y_trues\")\n",
    "    ax.plot(t, y_preds, lw=2., c='r', label=\"y_preds\")\n",
    "    ax.fill_between(t, y_preds-y_preds.std(), \n",
    "                    y_preds+y_preds.std(),\n",
    "                    facecolor='blue', alpha=0.5,\n",
    "                    label=\"std of \" + algorithm + \" preds.\"\n",
    "                   )\n",
    "    ax.legend()\n",
    "    run.log({\"std of \" + algorithm + \" preds\": wandb.Image(ax,  caption=\"std of\" + algorithm)})\n",
    "\n",
    "    run.log({\"std of \" + algorithm + \" preds.\": ax})\n",
    "    \n",
    "    return run\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(run, model, name, experiment_name):\n",
    "    \n",
    "    if name == \"rf\" or name == \"gbr\":\n",
    "        dump(model, os.path.join(wandb.run.dir,\n",
    "                          name + experiment_name + \".joblib\"))\n",
    "        \n",
    "    elif name == \"dnnr\":\n",
    "        model.save_weights(os.path.join(wandb.run.dir,\n",
    "                                        name + experiment_name + \".h5\"))\n",
    "    else:\n",
    "        model.save(os.path.join(wandb.run.dir,\n",
    "                                name + experiment_name + \".h5\"))\n",
    "    return run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_eval_a_nonsequential_regressor(x_train, y_train, x_val, y_val, x_test, \n",
    "                                         y_test, df_cmp, algorithm, data_type):\n",
    "    \n",
    "    # wandb initilization        \n",
    "    run = init_a_wandb(name=run_id, \n",
    "                       project=project,\n",
    "                       notes=\"Non-sequential Regressions Comparison for SuperOX \" + data_type, \n",
    "                       group=algorithm\n",
    "                      )\n",
    "    \n",
    "    if algorithm.lower() == \"rf\":\n",
    "\n",
    "        y_preds, features_importance_gini, \\\n",
    "        features_importance_perm, \\\n",
    "        features_importance_shap, model = train_eval_rf(x_train=x_train,\n",
    "                                                 y_train=y_train, \n",
    "                                                 x_test=x_test,\n",
    "                                                 y_test=y_test,\n",
    "                                                 name=data_type\n",
    "                                                )\n",
    "\n",
    "        \n",
    "    elif algorithm.lower() == \"gbr\":\n",
    "        \n",
    "        y_preds, features_importance_gini, \\\n",
    "        features_importance_perm, \\\n",
    "        features_importance_shap, model = train_eval_gbr(x_train=x_train,\n",
    "                                                  y_train=y_train, \n",
    "                                                  x_test=x_test,\n",
    "                                                  y_test=y_test,\n",
    "                                                  name=data_type\n",
    "                                                 )\n",
    "        \n",
    "    elif algorithm.lower() == \"dnnr\":\n",
    "        \n",
    "        dnn_reg = DnnReg(n_units=n_units, n_features=[x_train.shape[1]])\n",
    "        n_features=[x_train.shape[1]]\n",
    "        print(\"n_features=[x_train.shape[1]]:\", n_features)\n",
    "        \n",
    "        y_preds, features_importance_shap, model = train_eval_dnn_reg(dnn_reg=dnn_reg, \n",
    "                                                               x_train=x_train, \n",
    "                                                               y_train=y_train, \n",
    "                                                               x_test=x_test,\n",
    "                                                               y_test=y_test,\n",
    "                                                               x_val=x_val,\n",
    "                                                               y_val=y_val,\n",
    "                                                               name=data_type\n",
    "                                                              )\n",
    "        shap_indices = features_importance_shap[0]\n",
    "        shap_val = features_importance_shap[1]\n",
    "        shap_name = features_importance_shap[2]\n",
    "\n",
    "\n",
    "        df_cmp = final_comparison(df=df_cmp, \n",
    "                                   y_preds=y_preds,\n",
    "                                   y_trues=y_test, \n",
    "                                   name=algorithm+'-Shap',\n",
    "                                   data_name=data_type,\n",
    "                                   indices=shap_indices,\n",
    "                                   values_features_importance=shap_val,\n",
    "                                   name_important_features=shap_name\n",
    "                                  )\n",
    "        print(\" Shap is done!\")\n",
    "        \n",
    "    \n",
    "    run = wandb_metrics(run=run, y_trues=y_test, y_preds=y_preds)\n",
    "    run = wandb_plot_predictions(run=run, algorithm=algorithm, \n",
    "                                 y_trues=y_test, y_preds=y_preds)\n",
    "        \n",
    "    if algorithm.lower() == 'rf' or algorithm.lower() == 'gbr':\n",
    "        \n",
    "\n",
    "        gini_indices, gini_val, \\\n",
    "        gini_name = apply_gini(estimator_important_features=features_importance_gini,\n",
    "                               pld_complete=x_test, name=algorithm\n",
    "                              )\n",
    "\n",
    "\n",
    "        df_cmp = final_comparison(df=df_cmp, \n",
    "                                  y_preds=y_preds,\n",
    "                                  y_trues=y_test, \n",
    "                                  name=algorithm+'-Gini',\n",
    "                                  data_name=data_type,\n",
    "                                  indices=gini_indices,\n",
    "                                  values_features_importance=gini_val,\n",
    "                                  name_important_features=gini_name\n",
    "                                 )\n",
    "        \n",
    "        print(\"Gini is done!\")\n",
    "                \n",
    "        run = wandb_features_importance(run=run, \n",
    "                                        values_features_importance=gini_val,\n",
    "                                        name_important_features=gini_name,\n",
    "                                        indices_important_features=gini_indices,\n",
    "                                        importance_method=\"Gini\")\n",
    "\n",
    "#         run = wandb.Api().run(run_id)\n",
    "#         run.tags.append(\"Gini\")\n",
    "#         run.update()\n",
    "\n",
    "        perm_indices, perm_val, \\\n",
    "        perm_name = apply_permutation(estimator_important_features=features_importance_perm,\n",
    "                                      pld_complete=x_test, name=algorithm)\n",
    "\n",
    "        run = wandb_features_importance(run=run, \n",
    "                                        values_features_importance=perm_val,\n",
    "                                        name_important_features=perm_name,\n",
    "                                        indices_important_features=perm_indices,\n",
    "                                        importance_method=\"Perm\")\n",
    "\n",
    "#         run = wandb.Api().run(run_id)\n",
    "#         run.tags.append(\"Perm\")\n",
    "#         run.update()\n",
    "\n",
    "        df_cmp = final_comparison(df=df_cmp, \n",
    "                                  y_preds=y_preds,\n",
    "                                  y_trues=y_test, \n",
    "                                  name=algorithm+'-Perm',\n",
    "                                  data_name=data_type,\n",
    "                                  indices=perm_indices,\n",
    "                                  values_features_importance=perm_val,\n",
    "                                  name_important_features=perm_name\n",
    "                                 )\n",
    "        \n",
    "        print(\" Perm. is done!\")\n",
    "                \n",
    "        \n",
    "        shap_indices = features_importance_shap[0]\n",
    "        shap_val = features_importance_shap[1]\n",
    "        shap_name = features_importance_shap[2]\n",
    "        \n",
    "       \n",
    "\n",
    "        df_cmp = final_comparison(df=df_cmp, \n",
    "                                   y_preds=y_preds,\n",
    "                                   y_trues=y_test, \n",
    "                                   name=algorithm+'-Shap',\n",
    "                                   data_name=data_type,\n",
    "                                   indices=shap_indices,\n",
    "                                   values_features_importance=shap_val,\n",
    "                                   name_important_features=shap_name\n",
    "                                  )\n",
    "        print(\" Shap is done!\")\n",
    "    \n",
    "        run = wandb_features_importance(run=run, \n",
    "                                        values_features_importance=shap_val,\n",
    "                                        name_important_features=shap_name,\n",
    "                                        indices_important_features=shap_indices, \n",
    "                                        importance_method=\"Shap\")\n",
    "#         run = wandb.Api().run(run_id)\n",
    "#         run.tags.append(\"Shap\")\n",
    "#         run.update()\n",
    "    \n",
    "    run = save_model(run=run, model=model, \n",
    "                     name=algorithm.lower(), \n",
    "                     experiment_name=algorithm+\"-\"+data_type)\n",
    "\n",
    "\n",
    "    return df_cmp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# tmp = train_eval_a_nonsequential_regressor(x_train=x_r_train_real,\n",
    "#                          y_train=y_train_real, \n",
    "#                          x_val=x_r_val_real, \n",
    "#                          y_val=y_val_real,\n",
    "#                          x_test=x_r_test_real,\n",
    "#                          y_test=y_test_real, \n",
    "#                          df_cmp=df_cmp_real,\n",
    "#                          algorithm=\"RF\", \n",
    "#                          data_type=data_type)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tmp\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# tmp = train_eval_a_nonsequential_regressor(x_train=x_r_train_real,\n",
    "#                                            y_train=y_train_real, \n",
    "#                                            x_val=x_r_val_real,\n",
    "#                                            y_val=y_val_real,\n",
    "#                                            x_test=x_r_test_real, \n",
    "#                                            y_test=y_test_real, \n",
    "#                                            df_cmp=df_cmp_real,\n",
    "#                                            algorithm=\"GBR\", \n",
    "#                                            data_type=data_type)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tmp\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# tmp = train_eval_a_nonsequential_regressor(x_train=x_r_train_real, \n",
    "#                                       y_train=y_train_real, \n",
    "#                                       x_val=x_r_val_real,\n",
    "#                                       y_val=y_val_real,\n",
    "#                                       x_test=x_r_test_real,\n",
    "#                                       y_test=y_test_real, \n",
    "#                                       df_cmp=df_cmp_real,\n",
    "#                                       algorithm=\"DNNR\", \n",
    "#                                       data_type=data_type)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tmp\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_cmp_real = pd.read_csv(\"final_comparison-real-all.csv\")\n",
    "# df_cmp_real\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequential Regressions\n",
    "\n",
    "We will make a set of predictions based on a window of consecutive samples from the data. More precisely, we will implement various models --including Sequential Dense NN Reg. (SDNNR), Sequential Convolutional NN Reg. (SCNNR), and Recurrent NN Reg. (SRNNR) models -- and predict based on a window of consecutive samples. So let us first implement this Window.\n",
    "\n",
    "\n",
    "### Data windowing\n",
    "\n",
    "\n",
    "The features of the input windows are:\n",
    "\n",
    "- The width (number of time steps) of the input and label windows\n",
    "- The time offset between them.\n",
    "- Which features are used as inputs, labels, or both.\n",
    "- Single-output and multi-output predictions.\n",
    "- Single-time-step and multi-time-step predictions.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WindowGenerator():\n",
    "    def __init__(self, input_width, label_width, shift,\n",
    "                 train_df, val_df, test_df,\n",
    "                 label_columns=None):\n",
    "        \n",
    "        # Store the raw data.\n",
    "        self.train_df = train_df\n",
    "        self.val_df = val_df\n",
    "        self.test_df = test_df\n",
    "\n",
    "        # Work out the label column indices.\n",
    "        self.label_columns = label_columns\n",
    "        \n",
    "        if label_columns is not None:\n",
    "            self.label_columns_indices = {name: i for i, name in\n",
    "                                          enumerate(label_columns)}\n",
    "        \n",
    "        self.column_indices = {name: i for i, name in\n",
    "                               enumerate(train_df.columns)}\n",
    "\n",
    "        # Work out the window parameters.\n",
    "        self.input_width = input_width\n",
    "        self.label_width = label_width\n",
    "        self.shift = shift\n",
    "\n",
    "        self.total_window_size = input_width + shift\n",
    "\n",
    "        self.input_slice = slice(0, input_width)\n",
    "        self.input_indices = np.arange(self.total_window_size)[self.input_slice]\n",
    "\n",
    "        self.label_start = self.total_window_size - self.label_width\n",
    "        self.labels_slice = slice(self.label_start, None)\n",
    "        self.label_indices = np.arange(self.total_window_size)[self.labels_slice]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '\\n'.join([\n",
    "            f'Total window size: {self.total_window_size}',\n",
    "            f'Input indices: {self.input_indices}',\n",
    "            f'Label indices: {self.label_indices}',\n",
    "            f'Label column name(s): {self.label_columns}'])\n",
    "    \n",
    "    def split_window(self, features):\n",
    "        inputs = features[:, self.input_slice, :]\n",
    "        labels = features[:, self.labels_slice, :]\n",
    "        if self.label_columns is not None:\n",
    "            labels = tf.stack(\n",
    "                [labels[:, :, self.column_indices[name]] for name in self.label_columns],\n",
    "                axis=-1)\n",
    "\n",
    "      # Slicing doesn't preserve static shape information, so set the shapes\n",
    "      # manually. This way the `tf.data.Datasets` are easier to inspect.\n",
    "        inputs.set_shape([None, self.input_width, None])\n",
    "        labels.set_shape([None, self.label_width, None])\n",
    "        \n",
    "        return inputs, labels\n",
    "    \n",
    "    \n",
    "    def make_dataset(self, data):\n",
    "        data = np.array(data, dtype=np.float32)\n",
    "        ds = tf.keras.preprocessing.timeseries_dataset_from_array(\n",
    "            data=data,\n",
    "            targets=None,\n",
    "            sequence_length=self.total_window_size,\n",
    "            sequence_stride=1,\n",
    "            shuffle=True,\n",
    "            batch_size=32,)\n",
    "        \n",
    "        ds = ds.map(self.split_window)\n",
    "        return ds\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_windowed_tf_to_numpy(windowed_data_df):\n",
    "    \n",
    "    # Converting data TF to Numpy array for using in SHAP Lib. (Not very efficient way! :( )\n",
    "    # train data:\n",
    "    data_np_y = []\n",
    "    tmp_data = []\n",
    "    \n",
    "    for i, j in windowed_data_df:\n",
    "        data_np_y += j.numpy().flatten().tolist()\n",
    "        tmp_data.append(i.numpy())\n",
    "\n",
    "    bs, w, f = tmp_data[0].shape  # batch_size, window_size, n_features\n",
    "    n = sum([l.shape[0] for l in tmp_data])  # n_datapoints\n",
    "\n",
    "    data_np_x = np.empty(shape=(n, w, f))\n",
    "    interval = 0\n",
    "\n",
    "    for t in range(len(tmp_data)):\n",
    "        I, J, K = tmp_data[t].shape\n",
    "        for i in range(I):\n",
    "            for j in range(J):\n",
    "                for k in range(K):\n",
    "                    data_np_x[interval+i, j, k] = tmp_data[t][i, j, k]\n",
    "        interval += I\n",
    "        \n",
    "    return data_np_x, data_np_y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_and_fit(model, window):\n",
    "    \n",
    "    model.compile(loss=tf.losses.MeanSquaredError(),\n",
    "                  optimizer=tf.optimizers.Adam(),\n",
    "                  metrics=[tf.metrics.MeanAbsoluteError()])\n",
    "\n",
    "    history = model.fit(window.make_dataset(train_df), epochs=n_epochs,\n",
    "                        validation_data=window.make_dataset(val_df),\n",
    "                       )\n",
    "    \n",
    "    y_preds = model.predict(window.make_dataset(test_df))\n",
    "    y_preds = y_preds.flatten()\n",
    "    \n",
    "    x_train_np, y_train_np = convert_windowed_tf_to_numpy(window.make_dataset(train_df))\n",
    "    x_test_np, y_test_np = convert_windowed_tf_to_numpy(window.make_dataset(test_df))\n",
    "    \n",
    "    return history, y_preds, y_train_np, x_train_np, y_test_np, x_test_np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((11136, 163), (3713, 163), (3712, 163))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project_ =  project  #\"Sequential Regressions-all\"\n",
    "run_id_ = run_id\n",
    "data_type_ = data_type\n",
    "\n",
    "# uncomment the three line below for running on the entire dataset (not pilot)\n",
    "train_df = x_r_train_real_seq\n",
    "test_df = x_r_test_real_seq\n",
    "val_df = x_r_val_real_seq\n",
    "\n",
    "train_df.shape, test_df.shape, val_df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'increasing the window size for reduced-features -input_width=100, label_width=1, shift=1, winsize=101'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_id_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_eval_a_sequential_regressor(train_df,\n",
    "                                      val_df, \n",
    "                                      test_df,\n",
    "                                      input_width, \n",
    "                                      label_width,\n",
    "                                      shift,\n",
    "                                      algorithm, \n",
    "                                      df_cmp, \n",
    "                                      data_type,\n",
    "                                      label_columns):\n",
    "    \n",
    "    # wandb initilization\n",
    "    run = init_a_wandb(name=run_id_, \n",
    "                       project=project_,\n",
    "                       notes=\"Sequential Regressions Comparison for SuperOX \" + data_type, \n",
    "                       group=algorithm\n",
    "                      )\n",
    "    \n",
    "    \n",
    "    # Model instantiation\n",
    "    if algorithm == \"Baseline\":\n",
    "        print(\"Baseline is Running\")\n",
    "                \n",
    "        model = tfk.Sequential([\n",
    "            tfkl.Dense(units=1)\n",
    "        ])\n",
    "    \n",
    "    \n",
    "    if algorithm == \"SDNNR\":  # Dense NN with multi_step_window Reg.\n",
    "        print(\"Dense TD Reg.\")\n",
    "        \n",
    "        model = tfk.Sequential([\n",
    "            tfkl.Dense(units=int(.5*n_units), activation=tf.nn.leaky_relu),\n",
    "            tfkl.Dense(units=n_units, activation=tf.nn.leaky_relu),\n",
    "            tfkl.Dense(units=2*n_units, activation=tf.nn.leaky_relu,),\n",
    "            tfkl.Dense(units=4*n_units, activation=tf.nn.leaky_relu,), \n",
    "            tfkl.Dense(units=1)\n",
    "        ])\n",
    "        \n",
    "    if algorithm == \"SRNNR\":\n",
    "        print(\"Dense TD RNN Reg.\")\n",
    "        \n",
    "        model = tf.keras.models.Sequential([\n",
    "            # Shape [batch, time, features] => [batch, time, lstm_units]\n",
    "            tf.keras.layers.LSTM(n_units, return_sequences=True),\n",
    "            tf.keras.layers.LSTM(n_units, return_sequences=True),\n",
    "            # Shape => [batch, time, features]\n",
    "            tf.keras.layers.Dense(units=1)\n",
    "        ])\n",
    "        \n",
    "    \n",
    "    if algorithm == \"SCNNR\":\n",
    "        print(\"Dense TD CNN Reg.\")\n",
    "        \n",
    "        CONV_WIDTH = input_width\n",
    "        \n",
    "        model = tf.keras.Sequential([\n",
    "            tfkl.Conv1D(filters=n_units,\n",
    "                           kernel_size=(CONV_WIDTH,),\n",
    "                           activation='relu'),\n",
    "            tf.keras.layers.Dense(units=n_units, activation='relu'),\n",
    "            tf.keras.layers.Dense(units=1),\n",
    "        ])\n",
    "\n",
    "\n",
    "    \n",
    "    window = WindowGenerator(\n",
    "        input_width=input_width, label_width=label_width, shift=shift,\n",
    "        train_df=train_df, val_df=val_df, test_df=test_df,\n",
    "        label_columns=[label_columns])\n",
    "\n",
    "    print(\"window:\", window)\n",
    "\n",
    "\n",
    "    # Training and validation of the model\n",
    "    \n",
    "    history, y_preds_model, y_train_np, x_train_np, y_test_np, x_test_np = compile_and_fit(model, window)\n",
    "    \n",
    "    \n",
    "    print(\"y_preds_model:\", y_preds_model.shape, len(y_test_np))\n",
    "    \n",
    "    \n",
    "    run = wandb_metrics(run=run, y_trues=y_test_np, \n",
    "                        y_preds=y_preds_model)\n",
    "    \n",
    "    run = wandb_plot_predictions(run=run, algorithm=algorithm, \n",
    "                                 y_trues=y_test_np, y_preds=y_preds_model)\n",
    "        \n",
    "    # Currently, Shap Lib does not support sequential data. \n",
    "    # Although the author of Shap recommended slicing or reshaping \n",
    "    # the 3-dimensional data (batch, n_time_step, n_features) to 1-d (n_features) \n",
    "    # or 2-d (n_samples, n_features) data, still, I think his proposed solution is a) \n",
    "    # not correct (because the TF will reshape such incompatible data into incorrect format\n",
    "    # regarding its input layer during the training process; b) not applicable to many \n",
    "    # neural network architectures (say CNN, LSTM, etc.)\n",
    "\n",
    "    \"\"\" \n",
    "    explainer, features_importance_shap = apply_shap_summary_plot(model=model,\n",
    "                                                                  x_train=train_df, \n",
    "                                                                  y_train=None, \n",
    "                                                                  x_test=test_df, \n",
    "                                                                  y_test=None, \n",
    "                                                                  n_clusters=10,\n",
    "                                                                  model_name=algorithm, \n",
    "                                                                  data_type=data_type\n",
    "                                                                 )\n",
    "        \n",
    "    shap_indices = features_importance_shap[0]\n",
    "    shap_val = features_importance_shap[1]\n",
    "    shap_name = features_importance_shap[2]\n",
    "    print(\"SHAP here:\", shap_val)\n",
    "    print(\"names:\", shap_name)\n",
    "    print(\"indices:\", shap_indices)\n",
    "    \n",
    "    run = wandb_features_importance(run=run, \n",
    "                                    values_features_importance=shap_val,\n",
    "                                    name_important_features=shap_name,\n",
    "                                    indices_important_features=shap_indices, \n",
    "                                    importance_method=\"Shap\")\n",
    "    \"\"\"\n",
    "    \n",
    "    run = save_model(run=run, model=model, \n",
    "                     name=algorithm.lower(), \n",
    "                     experiment_name=algorithm+\"-\"+data_type)\n",
    "    \n",
    "    \n",
    "    tmp = [1, 2, 3, 4, 5, 6]\n",
    "    tmp_ = [\"NaN\", \"NaN\", \"NaN\", \"NaN\", \"NaN\", \"NaN\"]\n",
    "    \n",
    "    df_cmp = final_comparison(df=df_cmp, \n",
    "                              y_preds=y_preds_model,\n",
    "                              y_trues=y_test_np, \n",
    "                              name=algorithm+'-Shap',\n",
    "                              data_name=data_type,\n",
    "                              indices=tmp,  # shap_indices,\n",
    "                              values_features_importance=[tmp],  # shap_val, \n",
    "                              name_important_features=tmp_  # shap_name \n",
    "                             )\n",
    "    \n",
    "    model.save(\"saved_model/\" + algorithm + \"-\" +data_type + \".h5\")\n",
    "    \n",
    "    return df_cmp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**\n",
    "Baseline and SDNNR are both linear **single input step models** and the difference is that the latter uses more complex model.\n",
    "\n",
    "\n",
    "- (So we cannot increase the input parameters (as the y_preds will be n_test_samples \\times input_width), unless we increase them all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# tmp_ = train_eval_a_sequential_regressor(train_df=train_df, \n",
    "#                                          val_df=val_df,\n",
    "#                                          test_df=test_df,  \n",
    "#                                          input_width=100,  # the main difference\n",
    "#                                          label_width=1,  # the main difference\n",
    "#                                          shift=1,\n",
    "#                                          algorithm=\"Baseline\", \n",
    "#                                          df_cmp=df_cmp_real_seq,\n",
    "#                                          data_type=data_type_,\n",
    "#                                          label_columns=\"Ic_norm\", \n",
    "#                                         )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tmp_\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# tmp_ = train_eval_a_sequential_regressor(train_df=train_df, \n",
    "#                                          val_df=val_df,\n",
    "#                                          test_df=test_df,\n",
    "#                                          input_width=100,  # the main difference\n",
    "#                                          label_width=1,  # the main difference\n",
    "#                                          shift=1,\n",
    "#                                          algorithm=\"SDNNR\", \n",
    "#                                          df_cmp=df_cmp_real_seq,\n",
    "#                                          data_type=data_type_,\n",
    "#                                          label_columns=\"Ic_norm\", \n",
    "#                                         )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tmp_\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Multi-step dense:**\n",
    "\n",
    "A single-time-step model has no context for the current values of its inputs. It can't see how the input features are changing over time. To address this issue the model needs access to multiple time steps when making predictions:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'increasing the window size for reduced-features -input_width=100label_width=1 shift=1 winsize=101 n_epochs=2000'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from TF documentation\n",
    "LABEL_WIDTH = 1\n",
    "CONV_WIDTH = 100\n",
    "INPUT_WIDTH = LABEL_WIDTH + (CONV_WIDTH - 1)\n",
    "\n",
    "run_id_ = \"increasing the window size for reduced-features -input_width=\" + \\\n",
    "str(INPUT_WIDTH) + \"label_width=\" + str(LABEL_WIDTH) + \\\n",
    "\" shift=1 winsize=\" + str(INPUT_WIDTH + 1) + \" n_epochs=\" + str(n_epochs)  # \n",
    "run_id_\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 100)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LABEL_WIDTH, INPUT_WIDTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msorooshi\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.27 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.25<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">increasing the window size for reduced-features -input_width=100label_width=1 shift=1 winsize=101 n_epochs=2000</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/sorooshi/SuperOX\" target=\"_blank\">https://wandb.ai/sorooshi/SuperOX</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/sorooshi/SuperOX/runs/vxaqkkzp\" target=\"_blank\">https://wandb.ai/sorooshi/SuperOX/runs/vxaqkkzp</a><br/>\n",
       "                Run data is saved locally in <code>/home/soroosh/Desktop/SearchOX/ko-vstp-ml/notebooks/wandb/run-20210427_124847-vxaqkkzp</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dense TD CNN Reg.\n",
      "window: Total window size: 101\n",
      "Input indices: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95\n",
      " 96 97 98 99]\n",
      "Label indices: [100]\n",
      "Label column name(s): ['Ic_norm']\n",
      "Epoch 1/2000\n",
      "345/345 [==============================] - 5s 14ms/step - loss: 0.0800 - mean_absolute_error: 0.1319 - val_loss: 1.1222 - val_mean_absolute_error: 0.9578\n",
      "Epoch 2/2000\n",
      "345/345 [==============================] - 5s 13ms/step - loss: 0.0028 - mean_absolute_error: 0.0400 - val_loss: 0.9648 - val_mean_absolute_error: 0.8970\n",
      "Epoch 3/2000\n",
      "345/345 [==============================] - 5s 13ms/step - loss: 0.0021 - mean_absolute_error: 0.0357 - val_loss: 0.9398 - val_mean_absolute_error: 0.8956\n",
      "Epoch 4/2000\n",
      "345/345 [==============================] - 5s 13ms/step - loss: 0.0020 - mean_absolute_error: 0.0346 - val_loss: 0.7901 - val_mean_absolute_error: 0.8192\n",
      "Epoch 5/2000\n",
      "345/345 [==============================] - 5s 13ms/step - loss: 0.0014 - mean_absolute_error: 0.0284 - val_loss: 0.7887 - val_mean_absolute_error: 0.8225\n",
      "Epoch 6/2000\n",
      "345/345 [==============================] - 5s 13ms/step - loss: 0.0017 - mean_absolute_error: 0.0322 - val_loss: 0.6599 - val_mean_absolute_error: 0.7516\n",
      "Epoch 7/2000\n",
      "345/345 [==============================] - 5s 13ms/step - loss: 0.0014 - mean_absolute_error: 0.0297 - val_loss: 0.7502 - val_mean_absolute_error: 0.8088\n",
      "Epoch 8/2000\n",
      "345/345 [==============================] - 5s 13ms/step - loss: 0.0018 - mean_absolute_error: 0.0333 - val_loss: 0.6188 - val_mean_absolute_error: 0.7305\n",
      "Epoch 9/2000\n",
      "345/345 [==============================] - 4s 13ms/step - loss: 0.0011 - mean_absolute_error: 0.0253 - val_loss: 0.5819 - val_mean_absolute_error: 0.7053\n",
      "Epoch 10/2000\n",
      "345/345 [==============================] - 5s 13ms/step - loss: 0.0014 - mean_absolute_error: 0.0297 - val_loss: 0.4784 - val_mean_absolute_error: 0.6406\n",
      "Epoch 11/2000\n",
      "345/345 [==============================] - 5s 13ms/step - loss: 0.0011 - mean_absolute_error: 0.0265 - val_loss: 0.4798 - val_mean_absolute_error: 0.6553\n",
      "Epoch 12/2000\n",
      "345/345 [==============================] - 5s 13ms/step - loss: 0.0011 - mean_absolute_error: 0.0258 - val_loss: 0.3781 - val_mean_absolute_error: 0.5715\n",
      "Epoch 13/2000\n",
      "345/345 [==============================] - 4s 13ms/step - loss: 7.8170e-04 - mean_absolute_error: 0.0222 - val_loss: 0.4336 - val_mean_absolute_error: 0.6141\n",
      "Epoch 14/2000\n",
      "345/345 [==============================] - 5s 13ms/step - loss: 5.5618e-04 - mean_absolute_error: 0.0184 - val_loss: 0.4391 - val_mean_absolute_error: 0.6121\n",
      "Epoch 15/2000\n",
      "345/345 [==============================] - 5s 13ms/step - loss: 8.6489e-04 - mean_absolute_error: 0.0231 - val_loss: 0.3983 - val_mean_absolute_error: 0.5693\n",
      "Epoch 16/2000\n",
      "345/345 [==============================] - 5s 13ms/step - loss: 9.2253e-04 - mean_absolute_error: 0.0235 - val_loss: 0.3968 - val_mean_absolute_error: 0.5800\n",
      "Epoch 17/2000\n",
      "345/345 [==============================] - 4s 13ms/step - loss: 0.0012 - mean_absolute_error: 0.0262 - val_loss: 0.3926 - val_mean_absolute_error: 0.5773\n",
      "Epoch 18/2000\n",
      "345/345 [==============================] - 4s 13ms/step - loss: 7.4066e-04 - mean_absolute_error: 0.0212 - val_loss: 0.3344 - val_mean_absolute_error: 0.5338\n",
      "Epoch 19/2000\n",
      "238/345 [===================>..........] - ETA: 1s - loss: 0.0010 - mean_absolute_error: 0.0249"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "\n",
    "tmp_ = train_eval_a_sequential_regressor(train_df=train_df, \n",
    "                                         val_df=val_df,\n",
    "                                         test_df=test_df,\n",
    "                                         input_width=INPUT_WIDTH,  # the main difference \n",
    "                                         label_width=LABEL_WIDTH,  # the main difference\n",
    "                                         shift=1,\n",
    "                                         algorithm=\"SCNNR\", \n",
    "                                         df_cmp=df_cmp_real_seq,\n",
    "                                         data_type=data_type_,\n",
    "                                         label_columns=\"Ic_norm\", \n",
    "                                        )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run_id_: increasing the window size for reduced-features - input_width=100, label_width=100, shift=1 winsize=101, n_epochs=2000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3gaeag9t) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 19380<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2f2dad72e914ef491572f82b976e4fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 20.49MB of 20.49MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/home/soroosh/Desktop/SearchOX/ko-vstp-ml/notebooks/wandb/run-20210427_151657-3gaeag9t/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/home/soroosh/Desktop/SearchOX/ko-vstp-ml/notebooks/wandb/run-20210427_151657-3gaeag9t/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>MAE</td><td>0.05601</td></tr><tr><td>MRAE</td><td>0.0333</td></tr><tr><td>RMSE</td><td>0.07141</td></tr><tr><td>R^2-Score</td><td>-3.91233</td></tr><tr><td>_runtime</td><td>47923</td></tr><tr><td>_timestamp</td><td>1619573779</td></tr><tr><td>_step</td><td>2</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>MAE</td><td>▁</td></tr><tr><td>MRAE</td><td>▁</td></tr><tr><td>RMSE</td><td>▁</td></tr><tr><td>R^2-Score</td><td>▁</td></tr><tr><td>_runtime</td><td>▁██</td></tr><tr><td>_timestamp</td><td>▁██</td></tr><tr><td>_step</td><td>▁▅█</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 6 W&B file(s), 2 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">increasing the window size for reduced-features - input_width=100, label_width=100, shift=1 winsize=101, n_epochs=2000</strong>: <a href=\"https://wandb.ai/sorooshi/SuperOX/runs/3gaeag9t\" target=\"_blank\">https://wandb.ai/sorooshi/SuperOX/runs/3gaeag9t</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "...Successfully finished last run (ID:3gaeag9t). Initializing new run:<br/><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.27 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.25<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">increasing the window size for reduced-features - input_width=100, label_width=100, shift=1 winsize=101, n_epochs=2000</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/sorooshi/SuperOX\" target=\"_blank\">https://wandb.ai/sorooshi/SuperOX</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/sorooshi/SuperOX/runs/2x34njbm\" target=\"_blank\">https://wandb.ai/sorooshi/SuperOX/runs/2x34njbm</a><br/>\n",
       "                Run data is saved locally in <code>/home/soroosh/Desktop/SearchOX/ko-vstp-ml/notebooks/wandb/run-20210428_112030-2x34njbm</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dense TD RNN Reg.\n",
      "window: Total window size: 101\n",
      "Input indices: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95\n",
      " 96 97 98 99]\n",
      "Label indices: [  1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18\n",
      "  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36\n",
      "  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54\n",
      "  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72\n",
      "  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90\n",
      "  91  92  93  94  95  96  97  98  99 100]\n",
      "Label column name(s): ['Ic_norm']\n",
      "Epoch 1/2000\n",
      "345/345 [==============================] - 26s 69ms/step - loss: 0.1867 - mean_absolute_error: 0.2041 - val_loss: 0.0375 - val_mean_absolute_error: 0.1159\n",
      "Epoch 2/2000\n",
      "150/345 [============>.................] - ETA: 11s - loss: 0.0033 - mean_absolute_error: 0.0211"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "LABEL_WIDTH = 100\n",
    "INPUT_WIDTH = 100\n",
    "\n",
    "run_id_ = \"increasing the window size for reduced-features - input_width=\" + \\\n",
    "str(INPUT_WIDTH) + \", label_width=\" + str(LABEL_WIDTH) + \\\n",
    "\", shift=1 winsize=\" + str(INPUT_WIDTH + 1) + \", n_epochs=\" + str(n_epochs)  # \n",
    "\n",
    "print(\"run_id_:\", run_id_)\n",
    "\n",
    "tmp_ = train_eval_a_sequential_regressor(train_df=train_df, \n",
    "                                         val_df=val_df,\n",
    "                                         test_df=test_df,\n",
    "                                         input_width=LABEL_WIDTH,  # (24) the main difference\n",
    "                                         label_width=INPUT_WIDTH,  # (24) the main difference\n",
    "                                         shift=1,\n",
    "                                         algorithm=\"SRNNR\", \n",
    "                                         df_cmp=df_cmp_real_seq,\n",
    "                                         data_type=data_type_,\n",
    "                                         label_columns=\"Ic_norm\", \n",
    "                                        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MAE</th>\n",
       "      <th>MRAE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>R^2-Score</th>\n",
       "      <th>Value, Name-1</th>\n",
       "      <th>Value, Name-2</th>\n",
       "      <th>Value, Name-3</th>\n",
       "      <th>Value, Name-4</th>\n",
       "      <th>Value, Name-5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Baseline-Shap</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SDNNR-Shap</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SCNNR-Shap</th>\n",
       "      <td>4.923053</td>\n",
       "      <td>2.913155</td>\n",
       "      <td>5.019470</td>\n",
       "      <td>-23419.488654</td>\n",
       "      <td>(2.00000, NaN)</td>\n",
       "      <td>(3.00000, NaN)</td>\n",
       "      <td>(4.00000, NaN)</td>\n",
       "      <td>(5.00000, NaN)</td>\n",
       "      <td>(6.00000, NaN)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SRNNR-Shap</th>\n",
       "      <td>0.056006</td>\n",
       "      <td>0.033299</td>\n",
       "      <td>0.071406</td>\n",
       "      <td>-3.912331</td>\n",
       "      <td>(2.00000, NaN)</td>\n",
       "      <td>(3.00000, NaN)</td>\n",
       "      <td>(4.00000, NaN)</td>\n",
       "      <td>(5.00000, NaN)</td>\n",
       "      <td>(6.00000, NaN)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    MAE      MRAE      RMSE     R^2-Score   Value, Name-1  \\\n",
       "Baseline-Shap       NaN       NaN       NaN           NaN             NaN   \n",
       "SDNNR-Shap          NaN       NaN       NaN           NaN             NaN   \n",
       "SCNNR-Shap     4.923053  2.913155  5.019470 -23419.488654  (2.00000, NaN)   \n",
       "SRNNR-Shap     0.056006  0.033299  0.071406     -3.912331  (2.00000, NaN)   \n",
       "\n",
       "                Value, Name-2   Value, Name-3   Value, Name-4   Value, Name-5  \n",
       "Baseline-Shap             NaN             NaN             NaN             NaN  \n",
       "SDNNR-Shap                NaN             NaN             NaN             NaN  \n",
       "SCNNR-Shap     (3.00000, NaN)  (4.00000, NaN)  (5.00000, NaN)  (6.00000, NaN)  \n",
       "SRNNR-Shap     (3.00000, NaN)  (4.00000, NaN)  (5.00000, NaN)  (6.00000, NaN)  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run_id_: increasing the window size for reduced-features - input_width=100, label_width=100, shift=1 winsize=101, n_epochs=2000\n"
     ]
    }
   ],
   "source": [
    "print(\"run_id_:\", run_id_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion over real-only data:\n",
    "\n",
    "- All three algorithms obtained acceptable results w.r.t MAE, MRAE, RMSE.\n",
    "\n",
    "- Although all of these three also obtain acceptable r^2 score, however, RF is the winner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_windowed_dataset(X, y, time_steps):\n",
    "#     Xs, ys = [], []\n",
    "#     for i in range(len(X)-time_steps):\n",
    "#         v = X.iloc[i:(i+time_steps)].values\n",
    "#         Xs.append(v)\n",
    "#         ys.append(y.iloc[i+time_steps])\n",
    "#     return np.asarray(Xs), np.asarray(ys)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def windowed_dataset(X, y, time_steps):\n",
    "#     ds = tfk.preprocessing.timeseries_dataset_from_array(\n",
    "#         data=X,\n",
    "#         targets=y,\n",
    "#         sequence_length=time_steps,\n",
    "#         sequence_stride=1,\n",
    "#         shuffle=False,\n",
    "#     )\n",
    "#     return ds\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfGvenv",
   "language": "python",
   "name": "tfgvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
